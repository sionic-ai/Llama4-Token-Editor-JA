#!/usr/bin/env python
# -*- coding: utf-8 -*-
import unittest
import logging
import transformers
import os
from token_analyzer_jp import (
    # åˆ©ç”¨ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
    is_japanese_related_char,
    is_pure_japanese_script_char,
    is_special_char_pattern,
    # ãƒ¡ã‚¤ãƒ³åˆ†æé–¢æ•°
    analyze_token_categories,
    # å®šç¾©ã•ã‚ŒãŸæ–‡å­—ã‚»ãƒƒãƒˆï¼ˆãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã§å‚ç…§ã™ã‚‹ãŸã‚ï¼‰
    HIRAGANA,
    KATAKANA,
    KATAKANA_HW,
    KANJI_COMMON,
    KANJI_EXT_A,
    JP_PUNCT,
    JP_SYMBOLS_ETC,
    JP_FULLWIDTH_ASCII_PRINTABLE,
    ENGLISH_BASIC,
)

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ã®INFOãƒ¬ãƒ™ãƒ«ä»¥ä¸Šã®ãƒ­ã‚°ã¯æŠ‘åˆ¶ (å¿…è¦ã«å¿œã˜ã¦å¤‰æ›´)
# è©³ç´°ãªãƒ­ã‚°ã‚’è¦‹ãŸã„å ´åˆã¯ã€ä»¥ä¸‹ã®è¡Œã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã€å¿…è¦ã«å¿œã˜ã¦ãƒ¬ãƒ™ãƒ«èª¿æ•´
logging.disable(logging.CRITICAL)
# logging.basicConfig(level=logging.DEBUG) # ãƒ‡ãƒãƒƒã‚°æ™‚ã«æœ‰åŠ¹åŒ–

# --- å®šæ•° ---
# å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«IDï¼ˆãƒ¢ãƒƒã‚¯ã—ãªã„ï¼‰
TARGET_MODEL_ID = "unsloth/Llama-4-Scout-17B-16E-Instruct"
# ãƒ†ã‚¹ãƒˆå¯¾è±¡ã¨ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³IDã®é–‹å§‹ç‚¹ (ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¿ã‘ã‚‹)
MIN_TEST_TOKEN_ID = 102
# ãƒ†ã‚¹ãƒˆã§æ¤œè¨¼ã™ã‚‹ç‰¹å®šãƒˆãƒ¼ã‚¯ãƒ³IDã®ä¾‹ (ãƒ¢ãƒ‡ãƒ«ä¾å­˜)
# ã“ã‚Œã‚‰ã®IDã¯ TARGET_MODEL_ID ã«ãŠã„ã¦æœŸå¾…ã•ã‚Œã‚‹ã‚«ãƒ†ã‚´ãƒªã‚’æŒã¤ã‚‚ã®ã¨ã™ã‚‹
# ä¾‹: (ID, ãƒ‡ã‚³ãƒ¼ãƒ‰æ–‡å­—åˆ—(å‚è€ƒ), æœŸå¾…ã•ã‚Œã‚‹ã‚«ãƒ†ã‚´ãƒª(ãƒªã‚¹ãƒˆ), æœŸå¾…ã•ã‚Œãªã„ã‚«ãƒ†ã‚´ãƒª(ãƒªã‚¹ãƒˆ))
# æ³¨æ„: ãƒ¢ãƒ‡ãƒ«æ›´æ–°ã§IDãŒå¤‰ã‚ã‚‹å¯èƒ½æ€§ã‚ã‚Šã€‚ãƒ†ã‚¹ãƒˆå¤±æ•—æ™‚ã¯IDã‚’ç¢ºèªãƒ»æ›´æ–°ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚
#       ã‚¹ãƒšãƒ¼ã‚¹ã‚’å«ã‚€ãƒˆãƒ¼ã‚¯ãƒ³ã‚„ç‰¹æ®Šæ–‡å­—ã®åˆ†é¡ã¯ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ä¾å­˜æ€§ãŒé«˜ã„ã€‚
EXPECTED_TOKEN_CATEGORIES = [
    (
        30162,
        " æ—¥æœ¬",
        ["contains_japanese", "contains_kanji"],
        ["pure_japanese_script", "pure_english"],
    ),  # å…ˆé ­ã‚¹ãƒšãƒ¼ã‚¹ + æ¼¢å­—
    (
        31185,
        "èª",
        ["contains_japanese", "contains_kanji", "pure_japanese_script"],
        [],
    ),  # æ¼¢å­—ã®ã¿
    (
        30088,
        "ã§ã™",
        ["contains_japanese", "contains_hiragana", "pure_japanese_script"],
        [],
    ),  # ã²ã‚‰ãŒãªã®ã¿
    (
        30472,
        "ãƒˆãƒ¼ã‚¯ãƒ³",
        ["contains_japanese", "contains_katakana_full", "pure_japanese_script"],
        [],
    ),  # é•·éŸ³ç¬¦ã€Œãƒ¼ã€ã‚‚ã‚«ã‚¿ã‚«ãƒŠç¯„å›²(U+30FC)ã«å«ã¾ã‚Œã‚‹ãŸã‚ pure
    (
        105743,
        "ï½¶ï¾",
        ["contains_japanese", "contains_katakana_half", "pure_japanese_script"],
        [],
    ),  # åŠè§’ã‚«ã‚¿ã‚«ãƒŠ(æ¿ç‚¹ä»˜ã)
    (
        30004,
        "ã€",
        ["contains_japanese", "contains_jp_punct_symbol"],
        ["pure_japanese_script"],
    ),  # å¥èª­ç‚¹
    (
        99796,
        "ï¼¡ï¼¢ï¼£",
        ["contains_japanese", "contains_fullwidth_ascii"],
        ["pure_japanese_script"],
    ),  # å…¨è§’è‹±å­—
    (
        319,
        " a",
        ["contains_basic_english"],
        ["pure_english", "contains_japanese"],
    ),  # å…ˆé ­ã‚¹ãƒšãƒ¼ã‚¹ + è‹±å°æ–‡å­—
    (
        450,
        " Apple",
        ["contains_basic_english"],
        ["pure_english", "contains_japanese"],
    ),  # å…ˆé ­ã‚¹ãƒšãƒ¼ã‚¹ + è‹±å˜èª(å¤§æ–‡å­—é–‹å§‹)
    (
        13,
        " ",
        [],
        ["contains_japanese", "pure_english", "special_char_pattern"],
    ),  # ID 13 ã¯ Llama ç³»ã§ã‚¹ãƒšãƒ¼ã‚¹ -> special_idsã«å«ã¾ã‚Œã‚‹ã‹è¦ç¢ºèª
    (
        29900,
        " 123",
        ["contains_digit"],
        ["contains_japanese", "pure_english"],
    ),  # å…ˆé ­ã‚¹ãƒšãƒ¼ã‚¹ + æ•°å­—
    (30587, " Code", ["contains_basic_english"], []),  # å…ˆé ­ã‚¹ãƒšãƒ¼ã‚¹ + è‹±å˜èª
    (
        32100,
        "æ ªå¼ä¼šç¤¾",
        ["contains_japanese", "contains_kanji", "pure_japanese_script"],
        [],
    ),  # ä¼šç¤¾åï¼ˆæ¼¢å­—ã®ã¿ï¼‰
    (
        106324,
        "ChatGPT",
        ["contains_basic_english", "pure_english"],
        ["contains_japanese"],
    ),  # è‹±å˜èªï¼ˆå¤§æ–‡å­—å°æ–‡å­—æ··åœ¨ï¼‰
    (
        125933,
        "ãƒ»",
        ["contains_japanese", "contains_jp_punct_symbol"],
        ["pure_japanese_script"],
    ),  # ä¸­ç‚¹
    (
        30008,
        "ã€Œ",
        ["contains_japanese", "contains_jp_punct_symbol"],
        ["pure_japanese_script"],
    ),  # é‰¤æ‹¬å¼§é–‹å§‹
    (100, "<0x00>", [], []),  # Llama4 Scoutã§ã®ID 100ã€‚ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦é™¤å¤–ã•ã‚Œã‚‹ã¯ãš
    (2, "</s>", [], []),  # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³
    (
        29871,
        "\n",
        [],
        ["contains_japanese", "pure_english"],
    ),  # æ”¹è¡Œã‚³ãƒ¼ãƒ‰ã€‚ã‚¹ãƒšãƒ¼ã‚¹é¡ä¼¼æ‰±ã„ã‹ã€ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‹ï¼Ÿ
    (
        120128,
        " #",
        [],
        ["contains_japanese", "special_char_pattern"],
    ),  # ã‚¹ãƒšãƒ¼ã‚¹ + ç‰¹æ®Šæ–‡å­— '#' -> Uncategorized?
    (
        127991,
        " ğŸ”¥",
        [],
        ["contains_japanese", "special_char_pattern"],
    ),  # ã‚¹ãƒšãƒ¼ã‚¹ + çµµæ–‡å­— -> Uncategorized?
    (
        12756,
        " ---",
        [],
        ["contains_japanese", "contains_basic_english", "special_char_pattern"],
    ),  # ã‚¹ãƒšãƒ¼ã‚¹ + ãƒã‚¤ãƒ•ãƒ³ -> Uncategorized?
]


# ----- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã®ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ -----
class HelperFunctionTests(unittest.TestCase):
    def test_is_japanese_related_charã®æ‹¡å¼µã‚±ãƒ¼ã‚¹(self):
        # æ”¹å–„ã•ã‚ŒãŸ is_japanese_related_char é–¢æ•°ã‚’ãƒ†ã‚¹ãƒˆ
        test_cases = [
            # åŸºæœ¬
            ("ã‚", True),
            ("ã‚¢", True),
            ("æ¼¢", True),
            ("ãƒ¼", True),
            # åŠè§’ã‚«ã‚¿ã‚«ãƒŠ
            ("ï½¶", True),
            ("ï¾Ÿ", True),
            # å¥èª­ç‚¹ãƒ»è¨˜å·
            ("ã€‚", True),
            ("ã€€", True),
            ("ãƒ»", True),
            ("ï¿¥", True),
            ("ã€Œ", True),
            ("ï½¤", True),
            # å…¨è§’ASCII
            ("ï¼¡", True),
            ("ï½‚", True),
            ("ï¼", True),
            ("ï¼", True),
            ("ï½", True),
            # éè©²å½“
            ("A", False),
            ("1", False),
            ("$", False),
            (" ", False),
            ("\n", False),
            ("Î±", False),
        ]
        for char, expected in test_cases:
            with self.subTest(char=char, expected=expected):
                actual = is_japanese_related_char(char)
                self.assertEqual(
                    actual,
                    expected,
                    f"æ–‡å­— '{char}' (U+{ord(char):04X}) ã® is_japanese_related_char çµæœãŒæœŸå¾…å€¤ã¨ç•°ãªã‚Šã¾ã™",
                )

    def test_is_pure_japanese_script_charã®æ‹¡å¼µã‚±ãƒ¼ã‚¹(self):
        # æ”¹å–„ã•ã‚ŒãŸ is_pure_japanese_script_char é–¢æ•°ã‚’ãƒ†ã‚¹ãƒˆ (æœŸå¾…å€¤ä¿®æ­£æ¸ˆã¿)
        test_cases = [
            # è©²å½“
            ("ã‚", True),
            ("ã‚¢", True),
            ("æ¼¢", True),
            ("ï½¶", True),
            ("ãƒ¼", True),  # é•·éŸ³ç¬¦ (U+30FC) ã¯ã‚«ã‚¿ã‚«ãƒŠç¯„å›²å†…
            ("ï½¥", True),  # åŠè§’ä¸­ç‚¹ (U+FF65) ã¯åŠè§’ã‚«ã‚¿ã‚«ãƒŠç¯„å›²ã®é–‹å§‹ç‚¹
            # éè©²å½“
            ("ï¨‘", False),  # äº’æ›æ¼¢å­— (U+FA11) ã¯å®šç¾©ç¯„å›²å¤–
            ("ã€‚", False),
            ("ã€€", False),
            ("ï¼¡", False),
            ("1", False),
            (" ", False),
            ("\n", False),
        ]
        for char, expected in test_cases:
            with self.subTest(char=char, expected=expected):
                actual = is_pure_japanese_script_char(char)
                self.assertEqual(
                    actual,
                    expected,
                    f"æ–‡å­— '{char}' (U+{ord(char):04X}) ã® is_pure_japanese_script_char çµæœãŒæœŸå¾…å€¤ã¨ç•°ãªã‚Šã¾ã™",
                )

    def test_is_special_char_patternã®æ‹¡å¼µã‚±ãƒ¼ã‚¹(self):
        # æ”¹å–„ã•ã‚ŒãŸ is_special_char_pattern é–¢æ•°ã‚’ãƒ†ã‚¹ãƒˆ
        test_cases = [
            # è©²å½“ (è‹±æ•°ã€ç©ºç™½ã€å®šç¾©æ¸ˆã¿è¨€èªæ–‡å­—ä»¥å¤–ã®ã¿)
            ("!!!", True),
            ("@#$", True),
            ("&&&", True),
            ("+-*/", True),
            ("---", True),
            ("===", True),
            # éè©²å½“ (ä½•ã‹ãŒæ··ã–ã£ã¦ã„ã‚‹)
            ("abc", False),
            ("ã‚ã„ã†", False),
            ("123", False),
            ("ï¼¡ï¼¢ï¼£", False),
            ("ã‚«ã‚¿ã‚«ãƒŠ", False),
            ("åŠè§’ï½¶ï¾…", False),
            ("æ¼¢å­—", False),
            (" ", False),
            ("ã€€", False),
            ("a#$", False),
            ("#ã‚$", False),
            ("#1$", False),
            ("#ï¼¡$", False),
            ("", False),
            ("ã€Œã€", False),
            (" ---", False),  # ã‚¹ãƒšãƒ¼ã‚¹ãŒå«ã¾ã‚Œã‚‹ã¨ False
            (" #", False),  # ã‚¹ãƒšãƒ¼ã‚¹ãŒå«ã¾ã‚Œã‚‹ã¨ False
            (" ğŸ”¥", False),  # ã‚¹ãƒšãƒ¼ã‚¹ãŒå«ã¾ã‚Œã‚‹ã¨ False
        ]
        for token, expected in test_cases:
            with self.subTest(token=repr(token), expected=expected):
                actual = is_special_char_pattern(token)
                self.assertEqual(
                    actual,
                    expected,
                    f"ãƒˆãƒ¼ã‚¯ãƒ³ {repr(token)} ã® is_special_char_pattern çµæœãŒæœŸå¾…å€¤ã¨ç•°ãªã‚Šã¾ã™",
                )


# ----- ãƒ¡ã‚¤ãƒ³åˆ†æé–¢æ•°ã®ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ -----
# å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦åˆ†æçµæœã‚’æ¤œè¨¼
# @unittest.skipUnless(os.path.exists(TARGET_MODEL_ID.split("/")[-1]) or os.path.exists(TARGET_MODEL_ID) or "HUGGINGFACE_HUB_TOKEN" in os.environ,
#                   f"Requires local model at ./{TARGET_MODEL_ID.split('/')[-1]} or full path {TARGET_MODEL_ID}, or Hugging Face Hub token")
class AnalysisResultTests(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        # ã“ã®ã‚¯ãƒ©ã‚¹ã®å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œå‰ã«ä¸€åº¦ã ã‘å®Ÿè¡Œ
        print(f"\n--- {cls.__name__} ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–‹å§‹ ---")
        print(f"ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ¢ãƒ‡ãƒ«: {TARGET_MODEL_ID}")
        try:
            # å®Ÿéš›ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ (ãƒ¢ãƒƒã‚¯ä¸ä½¿ç”¨)
            cls.tokenizer = transformers.AutoTokenizer.from_pretrained(
                TARGET_MODEL_ID, trust_remote_code=True
            )
            print(f"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ ({cls.tokenizer.__class__.__name__}) ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†")

            # å®Ÿéš›ã®åˆ†æã‚’å®Ÿè¡Œ (æ™‚é–“ãŒã‹ã‹ã‚‹å¯èƒ½æ€§ã‚ã‚Š)
            print(f"ãƒˆãƒ¼ã‚¯ãƒ³åˆ†æã‚’é–‹å§‹ã—ã¾ã™ (min_token_id={MIN_TEST_TOKEN_ID})...")
            cls.result = analyze_token_categories(
                TARGET_MODEL_ID, min_token_id=MIN_TEST_TOKEN_ID
            )
            print("ãƒˆãƒ¼ã‚¯ãƒ³åˆ†æå®Œäº†")

            # çµæœã®å–å¾—ã¨åŸºæœ¬çš„ãªãƒã‚§ãƒƒã‚¯
            if cls.result is None:
                raise RuntimeError("analyze_token_categories ãŒ None ã‚’è¿”ã—ã¾ã—ãŸ")
            cls.stats = cls.result.get("statistics", {})
            cls.token_ids_by_category = cls.result.get("token_ids", {})
            cls.details = cls.result.get("analysis_details", {})
            print(f"åˆ†æå¯¾è±¡ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {cls.details.get('num_tokens_analyzed', 0):,}")

        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã®å‡¦ç†
            print(f"\n****** ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­ã«è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ ******")
            print(f"ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}")
            print(f"ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {e}")
            import traceback

            print(traceback.format_exc())  # è©³ç´°ãªãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚’å‡ºåŠ›
            print("***************************************************\n")
            cls.result = None  # ã‚¨ãƒ©ãƒ¼ãƒ•ãƒ©ã‚°
        finally:
            print(f"--- {cls.__name__} ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº† ---")

    def test_åˆ†æçµæœã®åŸºæœ¬æ§‹é€ ã¨å¿…é ˆã‚­ãƒ¼ã®å­˜åœ¨ç¢ºèª(self):
        # ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒæˆåŠŸã—ãŸã‹ç¢ºèª
        if self.result is None:
            self.fail("ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—ã®ãŸã‚ãƒ†ã‚¹ãƒˆå¼·åˆ¶çµ‚äº†")

        # ä¸»è¦ãªã‚­ãƒ¼ã®å­˜åœ¨ç¢ºèª
        self.assertEqual(self.result["model_id"], TARGET_MODEL_ID)
        self.assertIsInstance(self.result["vocab_size"], int)
        self.assertGreaterEqual(self.result["vocab_size"], 0)  # 0ä»¥ä¸Šã§ã‚ã‚‹ã“ã¨
        self.assertIsInstance(self.result["num_special_tokens"], int)
        self.assertGreaterEqual(self.result["num_special_tokens"], 0)

        expected_top_keys = [
            "model_id",
            "vocab_size",
            "num_special_tokens",
            "analysis_details",
            "statistics",
            "token_ids",
        ]
        for key in expected_top_keys:
            self.assertIn(
                key, self.result, f"å¿…é ˆã‚­ãƒ¼ '{key}' ãŒåˆ†æçµæœã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“"
            )

        # analysis_details ã®ã‚­ãƒ¼ç¢ºèª
        self.assertIsInstance(self.details, dict)
        expected_detail_keys = [
            "min_token_id_analyzed",
            "max_token_id_analyzed",
            "num_tokens_analyzed",
            "num_errors",
            "excluded_special_ids",
        ]
        for key in expected_detail_keys:
            self.assertIn(
                key,
                self.details,
                f"å¿…é ˆã‚­ãƒ¼ 'analysis_details.{key}' ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“",
            )
        self.assertIsInstance(self.details["excluded_special_ids"], list)

        # statistics ã¨ token_ids ã®ã‚­ãƒ¼ç¢ºèª (å®šç¾©ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªã¨ä¸€è‡´ã™ã‚‹ã‹)
        self.assertIsInstance(self.stats, dict)
        self.assertIsInstance(self.token_ids_by_category, dict)
        expected_category_keys = {
            "contains_japanese",
            "pure_japanese_script",
            "pure_english",
            "contains_hiragana",
            "contains_katakana_full",
            "contains_katakana_half",
            "contains_kanji",
            "contains_jp_punct_symbol",
            "contains_fullwidth_ascii",
            "contains_basic_english",
            "contains_digit",
            "special_char_pattern",
            "uncategorized",
        }
        self.assertEqual(
            set(self.stats.keys()),
            expected_category_keys,
            "statistics ã®ã‚­ãƒ¼ãŒæœŸå¾…ã•ã‚Œã‚‹ã‚«ãƒ†ã‚´ãƒªã¨ä¸€è‡´ã—ã¾ã›ã‚“",
        )
        self.assertEqual(
            set(self.token_ids_by_category.keys()),
            expected_category_keys,
            "token_ids ã®ã‚­ãƒ¼ãŒæœŸå¾…ã•ã‚Œã‚‹ã‚«ãƒ†ã‚´ãƒªã¨ä¸€è‡´ã—ã¾ã›ã‚“",
        )

        # åˆ†æå¯¾è±¡ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒ0ä»¥ä¸Šã§ã‚ã‚‹ã“ã¨
        self.assertGreaterEqual(self.details["num_tokens_analyzed"], 0)
        if self.details["num_tokens_analyzed"] == 0:
            logging.warning(
                "åˆ†æå¯¾è±¡ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒ0ã§ã—ãŸã€‚min_token_idã‚„ãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
            )

    def test_ã‚«ãƒ†ã‚´ãƒªé–“ã®éƒ¨åˆ†é›†åˆé–¢ä¿‚ã®æ¤œè¨¼(self):
        # ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒæˆåŠŸã—ãŸã‹ç¢ºèª
        if self.result is None:
            self.fail("ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—ã®ãŸã‚ãƒ†ã‚¹ãƒˆå¼·åˆ¶çµ‚äº†")

        # éƒ¨åˆ†é›†åˆé–¢ä¿‚ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹é–¢æ•°
        def check_subset(subset_name, superset_name):
            subset_set = set(self.token_ids_by_category.get(subset_name, []))
            superset_set = set(self.token_ids_by_category.get(superset_name, []))
            # ä¸¡æ–¹ã«è¦ç´ ãŒã‚ã‚‹å ´åˆã®ã¿ãƒã‚§ãƒƒã‚¯ (ç©ºé›†åˆåŒå£«ã¯å¸¸ã«éƒ¨åˆ†é›†åˆ)
            if subset_set and superset_set:
                self.assertTrue(
                    subset_set.issubset(superset_set),
                    f"'{subset_name}' ã¯ '{superset_name}' ã®éƒ¨åˆ†é›†åˆã§ã‚ã‚‹ã¹ãã§ã™",
                )

        # æ—¥æœ¬èªé–¢é€£ã®ãƒã‚§ãƒƒã‚¯
        check_subset("pure_japanese_script", "contains_japanese")
        check_subset("contains_hiragana", "contains_japanese")
        check_subset("contains_katakana_full", "contains_japanese")
        check_subset("contains_katakana_half", "contains_japanese")
        check_subset("contains_kanji", "contains_japanese")
        check_subset("contains_jp_punct_symbol", "contains_japanese")
        check_subset("contains_fullwidth_ascii", "contains_japanese")

        # è‹±èªé–¢é€£ã®ãƒã‚§ãƒƒã‚¯
        check_subset("pure_english", "contains_basic_english")

        # pure_japanese_script ã¨ä»–ã®æ—¥æœ¬èªè©³ç´°ã‚«ãƒ†ã‚´ãƒªã®é–¢ä¿‚
        pure_jp_set = set(self.token_ids_by_category.get("pure_japanese_script", []))
        jp_ps_set = set(self.token_ids_by_category.get("contains_jp_punct_symbol", []))
        fw_ascii_set = set(
            self.token_ids_by_category.get("contains_fullwidth_ascii", [])
        )

        # pure_jp ã«ã¯å¥èª­ç‚¹ã‚„å…¨è§’ASCIIã€Œã®ã¿ã€ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¯å«ã¾ã‚Œãªã„ã¯ãš
        if pure_jp_set and jp_ps_set:
            self.assertTrue(
                pure_jp_set.isdisjoint(jp_ps_set - pure_jp_set),
                "pure_japanese_script ã¨ (æ—¥æœ¬èªå¥èª­ç‚¹è¨˜å·ã®ã¿ã‚’å«ã‚€ãƒˆãƒ¼ã‚¯ãƒ³) ã¯æ’ä»–ã®ã¯ãšã§ã™",
            )
        if pure_jp_set and fw_ascii_set:
            self.assertTrue(
                pure_jp_set.isdisjoint(fw_ascii_set - pure_jp_set),
                "pure_japanese_script ã¨ (å…¨è§’ASCIIã®ã¿ã‚’å«ã‚€ãƒˆãƒ¼ã‚¯ãƒ³) ã¯æ’ä»–ã®ã¯ãšã§ã™",
            )

    def test_ã‚«ãƒ†ã‚´ãƒªé–“ã®æ’ä»–é–¢ä¿‚ã®æ¤œè¨¼(self):
        # ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒæˆåŠŸã—ãŸã‹ç¢ºèª
        if self.result is None:
            self.fail("ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—ã®ãŸã‚ãƒ†ã‚¹ãƒˆå¼·åˆ¶çµ‚äº†")

        # æ’ä»–é–¢ä¿‚ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹é–¢æ•°
        def check_disjoint(cat1_name, cat2_name):
            set1 = set(self.token_ids_by_category.get(cat1_name, []))
            set2 = set(self.token_ids_by_category.get(cat2_name, []))
            # ä¸¡æ–¹ã«è¦ç´ ãŒã‚ã‚‹å ´åˆã®ã¿ãƒã‚§ãƒƒã‚¯
            if set1 and set2:
                self.assertTrue(
                    set1.isdisjoint(set2),
                    f"ã‚«ãƒ†ã‚´ãƒª '{cat1_name}' ã¨ '{cat2_name}' ã¯æ’ä»–ã§ã‚ã‚‹ã¹ãã§ã™ (å…±é€šè¦ç´ : {set1.intersection(set2)})",
                )

        # ä¸»è¦ãªæ’ä»–é–¢ä¿‚ã®ãƒã‚§ãƒƒã‚¯
        check_disjoint("pure_japanese_script", "pure_english")
        check_disjoint("pure_japanese_script", "special_char_pattern")
        check_disjoint("pure_japanese_script", "uncategorized")

        check_disjoint(
            "pure_english", "contains_japanese"
        )  # pure_english ã¯æ—¥æœ¬èªã‚’å«ã¾ãªã„å®šç¾©
        check_disjoint("pure_english", "special_char_pattern")
        check_disjoint("pure_english", "uncategorized")

        check_disjoint("special_char_pattern", "contains_japanese")
        check_disjoint(
            "special_char_pattern", "contains_basic_english"
        )  # contains_basic_english ã¨ã‚‚æ’ä»–
        check_disjoint("special_char_pattern", "uncategorized")

        # æœªåˆ†é¡ã¯ä»–ã®ã€Œæ˜ç¢ºãªã€ã‚«ãƒ†ã‚´ãƒªã¨ã¯æ’ä»–
        uncat_set = set(self.token_ids_by_category.get("uncategorized", []))
        if uncat_set:
            defined_categories = [
                "contains_japanese",
                "pure_japanese_script",
                "pure_english",
                "contains_hiragana",
                "contains_katakana_full",
                "contains_katakana_half",
                "contains_kanji",
                "contains_jp_punct_symbol",
                "contains_fullwidth_ascii",
                "contains_basic_english",
                "contains_digit",
                "special_char_pattern",
            ]
            for cat_name in defined_categories:
                check_disjoint("uncategorized", cat_name)

    def test_ç‰¹å®šãƒˆãƒ¼ã‚¯ãƒ³IDã®ã‚«ãƒ†ã‚´ãƒªæ‰€å±æ¤œè¨¼(self):
        # ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒæˆåŠŸã—ãŸã‹ç¢ºèª
        if self.result is None:
            self.fail("ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—ã®ãŸã‚ãƒ†ã‚¹ãƒˆå¼·åˆ¶çµ‚äº†")

        # åˆ†æå¯¾è±¡IDç¯„å›²ã¨ç‰¹æ®ŠIDã‚’å–å¾—
        min_id = self.details["min_token_id_analyzed"]
        max_id = self.details["max_token_id_analyzed"]
        special_ids_set = set(self.details["excluded_special_ids"])

        # äº‹å‰å®šç¾©ãƒªã‚¹ãƒˆã‚’ä½¿ã£ã¦æ¤œè¨¼
        for (
            token_id,
            token_repr,
            expected_cats,
            not_expected_cats,
        ) in EXPECTED_TOKEN_CATEGORIES:
            # åˆ†æå¯¾è±¡å¤–ãƒ»ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã¯ã‚¹ã‚­ãƒƒãƒ—
            if not (min_id <= token_id <= max_id) or token_id in special_ids_set:
                continue

            # ãƒ‡ã‚³ãƒ¼ãƒ‰ (ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã¯è¨˜éŒ²)
            try:
                actual_decoded = self.tokenizer.decode(
                    [token_id], clean_up_tokenization_spaces=False
                )
            except Exception as e:
                actual_decoded = f"DECODE ERROR: {e}"

            # subTestã§å€‹åˆ¥ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã¨ã—ã¦å®Ÿè¡Œ
            with self.subTest(
                token_id=token_id,
                token_repr=repr(token_repr),
                actual_decoded=repr(actual_decoded),
            ):
                # æœŸå¾…ã‚«ãƒ†ã‚´ãƒªã¸ã®æ‰€å±ç¢ºèª
                for cat_name in expected_cats:
                    self.assertIn(
                        cat_name,
                        self.token_ids_by_category,
                        f"ãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: ã‚«ãƒ†ã‚´ãƒª '{cat_name}' ãŒçµæœã«å­˜åœ¨ã—ã¾ã›ã‚“",
                    )
                    cat_set = set(self.token_ids_by_category.get(cat_name, []))
                    self.assertIn(
                        token_id,
                        cat_set,
                        f"ID {token_id} ({repr(actual_decoded)}) ã¯ã‚«ãƒ†ã‚´ãƒª '{cat_name}' ã«ã€å«ã¾ã‚Œã‚‹ã¹ãã€‘ã§ã™",
                    )
                # éæœŸå¾…ã‚«ãƒ†ã‚´ãƒªã¸ã®éæ‰€å±ç¢ºèª
                for cat_name in not_expected_cats:
                    if (
                        cat_name in self.token_ids_by_category
                    ):  # çµæœã«ã‚«ãƒ†ã‚´ãƒªã‚­ãƒ¼ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ãƒã‚§ãƒƒã‚¯
                        cat_set = set(self.token_ids_by_category[cat_name])
                        self.assertNotIn(
                            token_id,
                            cat_set,
                            f"ID {token_id} ({repr(actual_decoded)}) ã¯ã‚«ãƒ†ã‚´ãƒª '{cat_name}' ã«ã€å«ã¾ã‚Œã‚‹ã¹ãã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‘",
                        )

    def test_çµ±è¨ˆå€¤ã®æ•´åˆæ€§æ¤œè¨¼(self):
        # ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒæˆåŠŸã—ãŸã‹ç¢ºèª
        if self.result is None:
            self.fail("ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—ã®ãŸã‚ãƒ†ã‚¹ãƒˆå¼·åˆ¶çµ‚äº†")

        # å„ã‚«ãƒ†ã‚´ãƒªã®çµ±è¨ˆæ•°ã¨å®Ÿéš›ã®IDãƒªã‚¹ãƒˆé•·ãŒä¸€è‡´ã™ã‚‹ã‹
        for name, count in self.stats.items():
            self.assertEqual(
                count,
                len(self.token_ids_by_category.get(name, [])),
                f"ã‚«ãƒ†ã‚´ãƒª '{name}' ã®çµ±è¨ˆæ•°({count})ã¨IDãƒªã‚¹ãƒˆé•·({len(self.token_ids_by_category.get(name, []))})ãŒä¸€è‡´ã—ã¾ã›ã‚“",
            )

        # åˆ†æã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³æ•° = ãƒ¦ãƒ‹ãƒ¼ã‚¯ã«åˆ†é¡ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³æ•° + æœªåˆ†é¡ãƒˆãƒ¼ã‚¯ãƒ³æ•°
        all_categorized_ids_union = set()
        for name, ids in self.token_ids_by_category.items():
            if name != "uncategorized":
                all_categorized_ids_union.update(ids)

        num_analyzed = self.details["num_tokens_analyzed"]
        num_uncategorized = self.stats["uncategorized"]
        num_categorized_unique = len(all_categorized_ids_union)

        # 0ä»¶ã®å ´åˆã‚‚å«ã‚ã¦ç­‰å¼ãŒæˆã‚Šç«‹ã¤ã‹æ¤œè¨¼
        self.assertEqual(
            num_analyzed,
            num_categorized_unique + num_uncategorized,
            f"åˆ†æãƒˆãƒ¼ã‚¯ãƒ³æ•°({num_analyzed:,})ãŒã€åˆ†é¡æ¸ˆã¿ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°({num_categorized_unique:,}) + æœªåˆ†é¡æ•°({num_uncategorized:,}) = {num_categorized_unique + num_uncategorized:,} ã¨ä¸€è‡´ã—ã¾ã›ã‚“",
        )


# --- ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ ---
if __name__ == "__main__":
    # verbosity=2 ã§å„ãƒ†ã‚¹ãƒˆãƒ¡ã‚½ãƒƒãƒ‰åã‚‚è¡¨ç¤º
    unittest.main(verbosity=2)
