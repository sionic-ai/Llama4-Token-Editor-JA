#!/usr/bin/env python
# -*- coding: utf-8 -*-
import unittest
import logging
import transformers
import os
from token_analyzer_jp import (
    is_japanese_related_char,
    is_pure_japanese_script_char,
    is_special_char_pattern,
    analyze_token_categories,
    HIRAGANA, KATAKANA, KATAKANA_HW, KANJI_COMMON, KANJI_EXT_A,
    JP_PUNCT, JP_SYMBOLS_ETC, JP_FULLWIDTH_ASCII_PRINTABLE, ENGLISH_BASIC
)

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ã®INFOãƒ¬ãƒ™ãƒ«ä»¥ä¸Šã®ãƒ­ã‚°ã¯æŠ‘åˆ¶ (å¿…è¦ã«å¿œã˜ã¦å¤‰æ›´)
logging.disable(logging.CRITICAL)
# logging.basicConfig(level=logging.INFO) # ãƒ‡ãƒãƒƒã‚°æ™‚ã«æœ‰åŠ¹åŒ–

TARGET_MODEL_ID = "unsloth/Llama-4-Scout-17B-16E-Instruct"
MIN_TEST_TOKEN_ID = 102
EXPECTED_TOKEN_CATEGORIES = [
    (30162, " æ—¥æœ¬", ["contains_japanese", "contains_kanji", "contains_basic_english"], ["pure_japanese_script", "pure_english"]), # å…ˆé ­ã‚¹ãƒšãƒ¼ã‚¹ã¯ basic_english ã«ã¯å«ã¾ã‚Œãªã„æƒ³å®šã ãŒã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ä¾å­˜ã§æŒ™å‹•ãŒå¤‰ã‚ã‚‹å¯èƒ½æ€§ã‚ã‚Šã€‚ä¸€æ—¦å«ã‚ãªã„æ–¹å‘ã§ãƒ†ã‚¹ãƒˆã€‚
    (31185, "èª", ["contains_japanese", "contains_kanji", "pure_japanese_script"], []),
    (30088, "ã§ã™", ["contains_japanese", "contains_hiragana", "pure_japanese_script"], []),
    (30472, "ãƒˆãƒ¼ã‚¯ãƒ³", ["contains_japanese", "contains_katakana_full", "pure_japanese_script"], []), # é•·éŸ³ç¬¦ã€Œãƒ¼ã€ã‚‚ã‚«ã‚¿ã‚«ãƒŠç¯„å›²(U+30FC)ã«å«ã¾ã‚Œã‚‹ãŸã‚ pure_japanese_script ã¨ãªã‚‹
    (105743, "ï½¶ï¾", ["contains_japanese", "contains_katakana_half", "pure_japanese_script"], []),
    (30004, "ã€", ["contains_japanese", "contains_jp_punct_symbol"], ["pure_japanese_script"]),
    (99796, "ï¼¡ï¼¢ï¼£", ["contains_japanese", "contains_fullwidth_ascii"], ["pure_japanese_script"]),
    (319, " a", ["contains_basic_english"], ["pure_english", "contains_japanese"]),
    (450, " Apple", ["contains_basic_english"], ["pure_english", "contains_japanese"]),
    (13, " ", [], ["contains_japanese", "pure_english", "special_char_pattern"]), # ID 13 ã¯ Llama ç³»ã§ã‚¹ãƒšãƒ¼ã‚¹ã®å ´åˆãŒå¤šã„ -> special_idsã«å«ã¾ã‚Œã‚‹ã¯ãšã ãŒç¢ºèª
    (29900, " 123", ["contains_digit"], ["contains_japanese", "pure_english"]),
    (30587, " Code", ["contains_basic_english"], []),
    (32100, "æ ªå¼ä¼šç¤¾", ["contains_japanese", "contains_kanji", "pure_japanese_script"], []),
    (106324, "ChatGPT", ["contains_basic_english", "pure_english"], ["contains_japanese"]),
    (125933, "ãƒ»", ["contains_japanese", "contains_jp_punct_symbol"], ["pure_japanese_script"]),
    (30008, "ã€Œ", ["contains_japanese", "contains_jp_punct_symbol"], ["pure_japanese_script"]),
    (100, " <0x00>", [], []), # ID 100 ã®å®Ÿéš›ã®ãƒ‡ã‚³ãƒ¼ãƒ‰æ–‡å­—åˆ—ã‚’ç¢ºèªã™ã‚‹å¿…è¦ã‚ã‚Š (Llama3ç³»ã§ã¯ <|reserved_special_token_0|> ãªã©) -> ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³
    (2, "</s>", [], []),
    (29871, "\n", [], ["contains_japanese", "pure_english"]), # æ”¹è¡Œã‚³ãƒ¼ãƒ‰ã‚‚é€šå¸¸ç‰¹æ®Šæ‰±ã„ã‹ã€ã‚¹ãƒšãƒ¼ã‚¹é¡ä¼¼æ‰±ã„ã‹
    (120128, " #", ["contains_basic_english", "special_char_pattern"], ["pure_english"]), # # ãŒ special_char_pattern ã«å±ã™ã‚‹ã‹ï¼Ÿ -> is_special_char_pattern('-') ã¯ True ã ãŒã€åˆ†é¡ãƒ­ã‚¸ãƒƒã‚¯ã§å¼¾ã‹ã‚Œã‚‹ã‹ï¼Ÿ
                                                                                    # '#'è‡ªä½“ã¯ASCIIæ–‡å­—ã ãŒisalnumã§ã‚‚isspaceã§ã‚‚ãªã„ã®ã§ã€is_special_char_pattern('#')ã¯Trueã€‚
                                                                                    # ãŸã ã— ' #' ãƒˆãƒ¼ã‚¯ãƒ³ã¯ã‚¹ãƒšãƒ¼ã‚¹ã‚’å«ã‚€ãŸã‚ is_special_char_pattern(' #') ã¯ False ã«ãªã‚‹ã¯ãšã€‚
                                                                                    # ã‚«ãƒ†ã‚´ãƒªåˆ†é¡ã§ã¯ contains_basic_english(ã‚¹ãƒšãƒ¼ã‚¹ç”±æ¥?) ã‚„ uncat ã«ãªã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã€‚è¦æ¤œè¨¼ã€‚
    (127991, " ğŸ”¥", [], ["contains_japanese", "special_char_pattern"]), # çµµæ–‡å­—ã¯ Uncategorized ã«ãªã‚‹å¯èƒ½æ€§ãŒé«˜ã„
    (12756, " ---", ["special_char_pattern"], ["contains_basic_english"]), # ' ---' ãƒˆãƒ¼ã‚¯ãƒ³ã€‚ã‚¹ãƒšãƒ¼ã‚¹ã‚’å«ã¾ãªã‘ã‚Œã° special_char_pattern ã ãŒã€å«ã‚€å ´åˆï¼Ÿ
                                                                          # is_special_char_pattern(' ---') ã¯ Falseã€‚Uncategorized å€™è£œã€‚
                                                                          # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã®æœŸå¾…å€¤ã‚’ Uncategorized ã«å¤‰æ›´ã™ã‚‹ã‹è¦æ¤œè¨ã€‚å…ƒã®ãƒ­ã‚°ã«å¾“ã„ special_char_pattern ã‚’æœŸå¾…ã€‚
]


class HelperFunctionTests(unittest.TestCase):

    def test_is_japanese_related_charã®æ‹¡å¼µã‚±ãƒ¼ã‚¹(self):
        test_cases = [
            ("ã‚", True), ("ã‚¢", True), ("æ¼¢", True), ("ãƒ¼", True),
            ("ï½¶", True), ("ï¾Ÿ", True),
            ("ã€‚", True), ("ã€€", True), ("ãƒ»", True), ("ï¿¥", True), ("ã€Œ", True), ("ï½¤", True),
            ("ï¼¡", True), ("ï½‚", True), ("ï¼", True), ("ï¼", True), ("ï½", True),
            ("A", False), ("1", False), ("$", False), (" ", False), ("\n", False), ("Î±", False)
        ]
        for char, expected in test_cases:
            with self.subTest(char=char, expected=expected):
                actual = is_japanese_related_char(char)
                self.assertEqual(actual, expected, f"æ–‡å­— '{char}' (U+{ord(char):04X}) ã® is_japanese_related_char çµæœãŒæœŸå¾…å€¤ã¨ç•°ãªã‚Šã¾ã™")

    def test_is_pure_japanese_script_charã®æ‹¡å¼µã‚±ãƒ¼ã‚¹(self):
        test_cases = [
            # è©²å½“
            ("ã‚", True), ("ã‚¢", True), ("æ¼¢", True), ("ï½¶", True),
            ("ãƒ¼", True), # é•·éŸ³ç¬¦ (U+30FC) ã¯ã‚«ã‚¿ã‚«ãƒŠç¯„å›²å†…ãªã®ã§ True (ä¿®æ­£)
            ("ï½¥", True), # åŠè§’ä¸­ç‚¹ (U+FF65) ã¯åŠè§’ã‚«ã‚¿ã‚«ãƒŠç¯„å›²ã®é–‹å§‹ç‚¹ãªã®ã§ True (ä¿®æ­£)
            # éè©²å½“
            ("ï¨‘", False), # äº’æ›æ¼¢å­— (U+FA11) ã¯å®šç¾©ç¯„å›²å¤–ãªã®ã§ False (ä¿®æ­£)
            ("ã€‚", False), ("ã€€", False), ("ï¼¡", False), ("1", False),
            (" ", False), ("\n", False)
        ]
        for char, expected in test_cases:
            with self.subTest(char=char, expected=expected):
                actual = is_pure_japanese_script_char(char)
                self.assertEqual(actual, expected, f"æ–‡å­— '{char}' (U+{ord(char):04X}) ã® is_pure_japanese_script_char çµæœãŒæœŸå¾…å€¤ã¨ç•°ãªã‚Šã¾ã™")

    def test_is_special_char_patternã®æ‹¡å¼µã‚±ãƒ¼ã‚¹(self):
        test_cases = [
            ("!!!", True), ("@#$", True), ("&&&", True), ("+-*/", True),
            ("abc", False), ("ã‚ã„ã†", False), ("123", False), ("ï¼¡ï¼¢ï¼£", False),
            ("ã‚«ã‚¿ã‚«ãƒŠ", False), ("åŠè§’ï½¶ï¾…", False), ("æ¼¢å­—", False),
            (" ", False), ("ã€€", False),
            ("a#$", False), ("#ã‚$", False), ("#1$", False), ("#ï¼¡$", False),
            ("", False),
            ("---", True), ("===", True), # isalnum, isspace, æ—¥æœ¬èªé–¢é€£, åŸºæœ¬è‹±èª ã®ã„ãšã‚Œã§ã‚‚ãªã„æ–‡å­—ã®ã¿
            ("ã€Œã€", False)
        ]
        for token, expected in test_cases:
             with self.subTest(token=repr(token), expected=expected):
                actual = is_special_char_pattern(token)
                self.assertEqual(actual, expected, f"ãƒˆãƒ¼ã‚¯ãƒ³ {repr(token)} ã® is_special_char_pattern çµæœãŒæœŸå¾…å€¤ã¨ç•°ãªã‚Šã¾ã™")

# --- AnalysisResultTests ã‚¯ãƒ©ã‚¹ã¯å¤‰æ›´ãªã— ---
# (ãŸã ã—ã€EXPECTED_TOKEN_CATEGORIES ã®æœŸå¾…å€¤ã¯å¿…è¦ã«å¿œã˜ã¦å†èª¿æ•´ãŒå¿…è¦ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹)
#@unittest.skipUnless(os.path.exists(TARGET_MODEL_ID.split("/")[-1]) or os.path.exists(TARGET_MODEL_ID) or "HUGGINGFACE_HUB_TOKEN" in os.environ,
#                   f"Requires local model at ./{TARGET_MODEL_ID.split('/')[-1]} or full path {TARGET_MODEL_ID}, or Hugging Face Hub token")
class AnalysisResultTests(unittest.TestCase):

    @classmethod
    def setUpClass(cls):
        print(f"\n--- {cls.__name__} ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–‹å§‹ ---")
        print(f"ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ¢ãƒ‡ãƒ«: {TARGET_MODEL_ID}")
        try:
            cls.tokenizer = transformers.AutoTokenizer.from_pretrained(
                TARGET_MODEL_ID, trust_remote_code=True
            )
            print("ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†")

            print(f"ãƒˆãƒ¼ã‚¯ãƒ³åˆ†æã‚’é–‹å§‹ã—ã¾ã™ (min_token_id={MIN_TEST_TOKEN_ID})...")
            # ãƒ†ã‚¹ãƒˆæ™‚é–“ã‚’çŸ­ç¸®ã—ãŸã„å ´åˆã¯ã€ä¸€æ™‚çš„ã«åˆ†æå¯¾è±¡ã‚’çµã‚‹
            # temp_max_id = 5000 # ä¾‹
            # cls.result = analyze_token_categories(TARGET_MODEL_ID, min_token_id=MIN_TEST_TOKEN_ID, max_token_id_override=temp_max_id)
            cls.result = analyze_token_categories(TARGET_MODEL_ID, min_token_id=MIN_TEST_TOKEN_ID)
            print("ãƒˆãƒ¼ã‚¯ãƒ³åˆ†æå®Œäº†")

            if cls.result is None:
                raise RuntimeError("analyze_token_categories ãŒ None ã‚’è¿”ã—ã¾ã—ãŸ")

            cls.stats = cls.result.get('statistics', {})
            cls.token_ids_by_category = cls.result.get('token_ids', {})
            cls.details = cls.result.get('analysis_details', {})

        except Exception as e:
            print(f"\n****** ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ ******")
            print(f"ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}")
            print(f"ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {e}")
            import traceback
            print(traceback.format_exc()) # è©³ç´°ãªãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚’å‡ºåŠ›
            print("***********************************************\n")
            cls.result = None

        print(f"--- {cls.__name__} ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº† ---")

    def test_åˆ†æçµæœã®åŸºæœ¬æ§‹é€ ã¨å¿…é ˆã‚­ãƒ¼ã®å­˜åœ¨ç¢ºèª(self):
        if self.result is None: self.fail("ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—ã®ãŸã‚ãƒ†ã‚¹ãƒˆå¼·åˆ¶çµ‚äº†") # failã«å¤‰æ›´
        # ä»¥ä¸‹ã®ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³ã¯å¤‰æ›´ãªã—
        self.assertEqual(self.result['model_id'], TARGET_MODEL_ID)
        self.assertGreater(self.result['vocab_size'], 0)
        expected_top_keys = ['model_id', 'vocab_size', 'num_special_tokens',
                             'analysis_details', 'statistics', 'token_ids']
        for key in expected_top_keys:
            self.assertIn(key, self.result, f"å¿…é ˆã‚­ãƒ¼ '{key}' ãŒåˆ†æçµæœã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
        expected_detail_keys = ['min_token_id_analyzed', 'max_token_id_analyzed',
                                'num_tokens_analyzed', 'num_errors', 'excluded_special_ids']
        for key in expected_detail_keys:
            self.assertIn(key, self.details, f"å¿…é ˆã‚­ãƒ¼ 'analysis_details.{key}' ãŒå«ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
        expected_category_keys = {
            "contains_japanese", "pure_japanese_script", "pure_english",
            "contains_hiragana", "contains_katakana_full", "contains_katakana_half",
            "contains_kanji", "contains_jp_punct_symbol", "contains_fullwidth_ascii",
            "contains_basic_english", "contains_digit", "special_char_pattern", "uncategorized"
        }
        self.assertEqual(set(self.stats.keys()), expected_category_keys,
                         "statistics ã®ã‚­ãƒ¼ãŒæœŸå¾…ã•ã‚Œã‚‹ã‚«ãƒ†ã‚´ãƒªã¨ä¸€è‡´ã—ã¾ã›ã‚“")
        self.assertEqual(set(self.token_ids_by_category.keys()), expected_category_keys,
                         "token_ids ã®ã‚­ãƒ¼ãŒæœŸå¾…ã•ã‚Œã‚‹ã‚«ãƒ†ã‚´ãƒªã¨ä¸€è‡´ã—ã¾ã›ã‚“")
        # ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦ã¯åˆ†æå¯¾è±¡ãƒˆãƒ¼ã‚¯ãƒ³ãŒ0ã«ãªã‚‹ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ã‚‚è€ƒãˆã‚‰ã‚Œã‚‹ãŸã‚ã€assertGreaterã‹ã‚‰å¤‰æ›´
        # self.assertGreater(self.details['num_tokens_analyzed'], 0, "åˆ†æå¯¾è±¡ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒ0ã§ã™")
        if self.details['num_tokens_analyzed'] == 0:
             logging.warning("åˆ†æå¯¾è±¡ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒ0ã§ã—ãŸã€‚min_token_idã‚„ãƒ¢ãƒ‡ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

    def test_ã‚«ãƒ†ã‚´ãƒªé–“ã®éƒ¨åˆ†é›†åˆé–¢ä¿‚ã®æ¤œè¨¼(self):
        if self.result is None: self.fail("ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—ã®ãŸã‚ãƒ†ã‚¹ãƒˆå¼·åˆ¶çµ‚äº†")
        # ä»¥ä¸‹ã®ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³ã¯å¤‰æ›´ãªã—
        def get_set(category_name): return set(self.token_ids_by_category.get(category_name, []))
        contains_jp = get_set("contains_japanese")
        pure_jp = get_set("pure_japanese_script")
        hira = get_set("contains_hiragana")
        kata_f = get_set("contains_katakana_full")
        kata_h = get_set("contains_katakana_half")
        kanji = get_set("contains_kanji")
        jp_ps = get_set("contains_jp_punct_symbol")
        fw_ascii = get_set("contains_fullwidth_ascii")
        contains_en = get_set("contains_basic_english")
        pure_en = get_set("pure_english")
        self.assertTrue(pure_jp.issubset(contains_jp))
        self.assertTrue(hira.issubset(contains_jp))
        self.assertTrue(kata_f.issubset(contains_jp))
        self.assertTrue(kata_h.issubset(contains_jp))
        self.assertTrue(kanji.issubset(contains_jp))
        self.assertTrue(jp_ps.issubset(contains_jp))
        self.assertTrue(fw_ascii.issubset(contains_jp))
        self.assertTrue(pure_en.issubset(contains_en))
        # isdisjoint ã®ãƒ†ã‚¹ãƒˆã‚‚ç¶­æŒ
        if pure_jp and jp_ps: # ä¸¡ã‚«ãƒ†ã‚´ãƒªã«ãƒˆãƒ¼ã‚¯ãƒ³ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ãƒã‚§ãƒƒã‚¯
             self.assertTrue(pure_jp.isdisjoint(jp_ps - pure_jp))
        if pure_jp and fw_ascii:
             self.assertTrue(pure_jp.isdisjoint(fw_ascii - pure_jp))

    def test_ã‚«ãƒ†ã‚´ãƒªé–“ã®æ’ä»–é–¢ä¿‚ã®æ¤œè¨¼(self):
        if self.result is None: self.fail("ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—ã®ãŸã‚ãƒ†ã‚¹ãƒˆå¼·åˆ¶çµ‚äº†")
        # ä»¥ä¸‹ã®ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³ã¯å¤‰æ›´ãªã—
        def get_set(category_name): return set(self.token_ids_by_category.get(category_name, []))
        pure_jp = get_set("pure_japanese_script")
        pure_en = get_set("pure_english")
        special = get_set("special_char_pattern")
        uncat = get_set("uncategorized")
        contains_jp = get_set("contains_japanese")
        contains_en = get_set("contains_basic_english")
        # æ’ä»–æ€§ã®ãƒã‚§ãƒƒã‚¯ (ç©ºé›†åˆã§ãªã„å ´åˆã®ã¿æ„å‘³ãŒã‚ã‚‹)
        if pure_jp and pure_en: self.assertTrue(pure_jp.isdisjoint(pure_en))
        if pure_jp and special: self.assertTrue(pure_jp.isdisjoint(special))
        if pure_jp and uncat: self.assertTrue(pure_jp.isdisjoint(uncat))
        if pure_en and contains_jp: self.assertTrue(pure_en.isdisjoint(contains_jp))
        if pure_en and special: self.assertTrue(pure_en.isdisjoint(special))
        if pure_en and uncat: self.assertTrue(pure_en.isdisjoint(uncat))
        if special and contains_jp: self.assertTrue(special.isdisjoint(contains_jp))
        if special and contains_en: self.assertTrue(special.isdisjoint(contains_en))
        if special and uncat: self.assertTrue(special.isdisjoint(uncat))
        # uncat ã¨ä»–ã®å…¨ã¦ã®ã‚«ãƒ†ã‚´ãƒªã®æ’ä»–æ€§ãƒã‚§ãƒƒã‚¯
        all_defined_categories = [ "contains_japanese", "pure_japanese_script", "pure_english", "contains_hiragana",
                                   "contains_katakana_full", "contains_katakana_half", "contains_kanji", "contains_jp_punct_symbol",
                                   "contains_fullwidth_ascii", "contains_basic_english", "contains_digit", "special_char_pattern" ]
        if uncat: # æœªåˆ†é¡ãƒˆãƒ¼ã‚¯ãƒ³ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ãƒã‚§ãƒƒã‚¯
            for cat_name in all_defined_categories:
                cat_set = get_set(cat_name)
                if cat_set: # å¯¾è±¡ã‚«ãƒ†ã‚´ãƒªã«ã‚‚ãƒˆãƒ¼ã‚¯ãƒ³ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ãƒã‚§ãƒƒã‚¯
                    self.assertTrue(uncat.isdisjoint(cat_set), f"uncategorized ã¨ {cat_name} ã¯æ’ä»–ã§ã‚ã‚‹ã¹ãã§ã™ (å…±é€šè¦ç´ : {uncat.intersection(cat_set)})")

    def test_ç‰¹å®šãƒˆãƒ¼ã‚¯ãƒ³IDã®ã‚«ãƒ†ã‚´ãƒªæ‰€å±æ¤œè¨¼(self):
        if self.result is None: self.fail("ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—ã®ãŸã‚ãƒ†ã‚¹ãƒˆå¼·åˆ¶çµ‚äº†")
        # ä»¥ä¸‹ã®ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³ã¯å¤‰æ›´ãªã— (EXPECTED_TOKEN_CATEGORIES ã®æœŸå¾…å€¤ã¯è¦èª¿æ•´)
        analyzed_ids_set = set(range(self.details['min_token_id_analyzed'],
                                   self.details['max_token_id_analyzed'] + 1))
        special_ids_set = set(self.details['excluded_special_ids'])

        for token_id, token_repr, expected_cats, not_expected_cats in EXPECTED_TOKEN_CATEGORIES:
            if token_id < self.details['min_token_id_analyzed'] or \
               token_id > self.details['max_token_id_analyzed'] or \
               token_id in special_ids_set:
                # print(f"æƒ…å ±: ãƒˆãƒ¼ã‚¯ãƒ³ID {token_id} ({repr(token_repr)}) ã¯åˆ†æå¯¾è±¡å¤–ã¾ãŸã¯ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                continue

            # ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦å®Ÿéš›ã®æ–‡å­—åˆ—ã‚’ç¢ºèª (ãƒ‡ãƒãƒƒã‚°ç”¨)
            try:
                actual_decoded = self.tokenizer.decode([token_id], clean_up_tokenization_spaces=False)
            except Exception as e:
                actual_decoded = f"DECODE ERROR: {e}"

            with self.subTest(token_id=token_id, token_repr=repr(token_repr), actual_decoded=repr(actual_decoded)):
                for cat_name in expected_cats:
                    self.assertIn(cat_name, self.token_ids_by_category)
                    cat_set = set(self.token_ids_by_category[cat_name])
                    self.assertIn(token_id, cat_set,
                                  f"ID {token_id} ({repr(actual_decoded)}) ã¯ '{cat_name}' ã«å«ã¾ã‚Œã‚‹ã¹ãã§ã™")
                for cat_name in not_expected_cats:
                     self.assertIn(cat_name, self.token_ids_by_category)
                     cat_set = set(self.token_ids_by_category[cat_name])
                     self.assertNotIn(token_id, cat_set,
                                     f"ID {token_id} ({repr(actual_decoded)}) ã¯ '{cat_name}' ã«å«ã¾ã‚Œã‚‹ã¹ãã§ã¯ã‚ã‚Šã¾ã›ã‚“")

    def test_çµ±è¨ˆå€¤ã®æ•´åˆæ€§æ¤œè¨¼(self):
        if self.result is None: self.fail("ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—ã®ãŸã‚ãƒ†ã‚¹ãƒˆå¼·åˆ¶çµ‚äº†")
        # ä»¥ä¸‹ã®ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³ã¯å¤‰æ›´ãªã—
        for name, count in self.stats.items():
            self.assertEqual(count, len(self.token_ids_by_category.get(name, [])),
                             f"ã‚«ãƒ†ã‚´ãƒª '{name}' ã®çµ±è¨ˆæ•°({count})ã¨IDãƒªã‚¹ãƒˆé•·({len(self.token_ids_by_category.get(name, []))})ãŒä¸€è‡´ã—ã¾ã›ã‚“")
        all_categorized_ids_union = set()
        for name, ids in self.token_ids_by_category.items():
             if name != 'uncategorized':
                 all_categorized_ids_union.update(ids)
        num_analyzed = self.details['num_tokens_analyzed']
        num_uncategorized = self.stats['uncategorized']
        num_categorized_unique = len(all_categorized_ids_union)
        # åˆ†æå¯¾è±¡ãŒãªã„å ´åˆã¯ num_analyzed=0, num_categorized_unique=0, num_uncategorized=0 ã¨ãªã‚‹ã¯ãš
        if num_analyzed > 0 or num_categorized_unique > 0 or num_uncategorized > 0:
             self.assertEqual(num_analyzed, num_categorized_unique + num_uncategorized,
                             f"åˆ†æãƒˆãƒ¼ã‚¯ãƒ³æ•°({num_analyzed})ãŒã€åˆ†é¡æ¸ˆã¿ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°({num_categorized_unique}) + æœªåˆ†é¡æ•°({num_uncategorized}) = {num_categorized_unique + num_uncategorized} ã¨ä¸€è‡´ã—ã¾ã›ã‚“")
        else:
             # åˆ†æå¯¾è±¡ãŒ0ä»¶ã®å ´åˆã‚‚ãƒ†ã‚¹ãƒˆã¯ãƒ‘ã‚¹ã¨ã™ã‚‹
             pass

if __name__ == "__main__":
    unittest.main(verbosity=2)