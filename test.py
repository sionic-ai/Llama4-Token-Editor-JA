#!/usr/bin/env python
# -*- coding: utf-8 -*-
import unittest
import logging
import transformers
import os
import random  # ã‚µãƒ³ãƒ—ãƒ«æŠ½å‡ºã«ä½¿ç”¨
from token_analyzer_jp import (
    # åˆ©ç”¨ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
    is_japanese_related_char,
    is_pure_japanese_script_char,
    is_special_char_pattern,
    # ãƒ¡ã‚¤ãƒ³åˆ†æé–¢æ•°
    analyze_token_categories,
    # å®šç¾©ã•ã‚ŒãŸæ–‡å­—ã‚»ãƒƒãƒˆï¼ˆåˆ†é¡ãƒ­ã‚¸ãƒƒã‚¯å†ç¾ã®ãŸã‚ï¼‰
    HIRAGANA,
    KATAKANA,
    KATAKANA_HW,
    KANJI_COMMON,
    KANJI_EXT_A,
    JP_PUNCT,
    JP_SYMBOLS_ETC,
    JP_FULLWIDTH_ASCII_PRINTABLE,
    ENGLISH_BASIC,
)

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ã®INFOãƒ¬ãƒ™ãƒ«ä»¥ä¸Šã®ãƒ­ã‚°ã¯æŠ‘åˆ¶ (å¿…è¦ã«å¿œã˜ã¦å¤‰æ›´)
# è©³ç´°ãªãƒ­ã‚°ã‚’è¦‹ãŸã„å ´åˆã¯ã€ä»¥ä¸‹ã®è¡Œã‚’ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã€å¿…è¦ã«å¿œã˜ã¦ãƒ¬ãƒ™ãƒ«èª¿æ•´
logging.disable(logging.CRITICAL)
# logging.basicConfig(level=logging.DEBUG) # ãƒ‡ãƒãƒƒã‚°æ™‚ã«æœ‰åŠ¹åŒ–

# --- å®šæ•° ---
# å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«IDï¼ˆãƒ¢ãƒƒã‚¯ã—ãªã„ï¼‰
TARGET_MODEL_ID = "unsloth/Llama-4-Scout-17B-16E-Instruct"
# ãƒ†ã‚¹ãƒˆå¯¾è±¡ã¨ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³IDã®é–‹å§‹ç‚¹ (ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¿ã‘ã‚‹)
MIN_TEST_TOKEN_ID = 102
# ã‚µãƒ³ãƒ—ãƒ«æ¤œè¨¼ã§æŠ½å‡ºã™ã‚‹IDã®æœ€å¤§æ•°
MAX_SAMPLES_PER_CATEGORY = 5  # å„ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰æœ€å¤§5ä»¶
TOTAL_SAMPLES_FOR_LOGIC_TEST = 50  # å…¨ä½“ã§æœ€å¤§50ä»¶ç¨‹åº¦


# ----- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã®ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ -----
class HelperFunctionTests(unittest.TestCase):
    def test_is_japanese_related_charã®æ‹¡å¼µã‚±ãƒ¼ã‚¹(self):
        # æ”¹å–„ã•ã‚ŒãŸ is_japanese_related_char é–¢æ•°ã‚’ãƒ†ã‚¹ãƒˆ
        test_cases = [
            # åŸºæœ¬
            ("ã‚", True),
            ("ã‚¢", True),
            ("æ¼¢", True),
            ("ãƒ¼", True),
            # åŠè§’ã‚«ã‚¿ã‚«ãƒŠ
            ("ï½¶", True),
            ("ï¾Ÿ", True),
            # å¥èª­ç‚¹ãƒ»è¨˜å·
            ("ã€‚", True),
            ("ã€€", True),
            ("ãƒ»", True),
            ("ï¿¥", True),
            ("ã€Œ", True),
            ("ï½¤", True),
            # å…¨è§’ASCII
            ("ï¼¡", True),
            ("ï½‚", True),
            ("ï¼", True),
            ("ï¼", True),
            ("ï½", True),
            # éè©²å½“
            ("A", False),
            ("1", False),
            ("$", False),
            (" ", False),
            ("\n", False),
            ("Î±", False),
        ]
        for char, expected in test_cases:
            with self.subTest(char=char, expected=expected):
                actual = is_japanese_related_char(char)
                self.assertEqual(
                    actual,
                    expected,
                    f"æ–‡å­— '{char}' (U+{ord(char):04X}) ã® is_japanese_related_char çµæœãŒæœŸå¾…å€¤ã¨ç•°ãªã‚Šã¾ã™",
                )

    def test_is_pure_japanese_script_charã®æ‹¡å¼µã‚±ãƒ¼ã‚¹(self):
        # æ”¹å–„ã•ã‚ŒãŸ is_pure_japanese_script_char é–¢æ•°ã‚’ãƒ†ã‚¹ãƒˆ (æœŸå¾…å€¤ä¿®æ­£æ¸ˆã¿)
        test_cases = [
            # è©²å½“
            ("ã‚", True),
            ("ã‚¢", True),
            ("æ¼¢", True),
            ("ï½¶", True),
            ("ãƒ¼", True),
            ("ï½¥", True),
            # éè©²å½“
            ("ï¨‘", False),
            ("ã€‚", False),
            ("ã€€", False),
            ("ï¼¡", False),
            ("1", False),
            (" ", False),
            ("\n", False),
        ]
        for char, expected in test_cases:
            with self.subTest(char=char, expected=expected):
                actual = is_pure_japanese_script_char(char)
                self.assertEqual(
                    actual,
                    expected,
                    f"æ–‡å­— '{char}' (U+{ord(char):04X}) ã® is_pure_japanese_script_char çµæœãŒæœŸå¾…å€¤ã¨ç•°ãªã‚Šã¾ã™",
                )

    def test_is_special_char_patternã®æ‹¡å¼µã‚±ãƒ¼ã‚¹(self):
        # æ”¹å–„ã•ã‚ŒãŸ is_special_char_pattern é–¢æ•°ã‚’ãƒ†ã‚¹ãƒˆ (æœŸå¾…å€¤ä¿®æ­£æ¸ˆã¿)
        test_cases = [
            # è©²å½“ (è‹±æ•°ã€ç©ºç™½ã€å®šç¾©æ¸ˆã¿è¨€èªæ–‡å­—ä»¥å¤–ã®ã¿)
            ("!!!", True),
            ("@#$", True),
            ("&&&", True),
            ("+-*/", True),
            ("---", True),
            ("===", True),
            # éè©²å½“ (ä½•ã‹ãŒæ··ã–ã£ã¦ã„ã‚‹)
            ("abc", False),
            ("ã‚ã„ã†", False),
            ("123", False),
            ("ï¼¡ï¼¢ï¼£", False),
            ("ã‚«ã‚¿ã‚«ãƒŠ", False),
            ("åŠè§’ï½¶ï¾…", False),
            ("æ¼¢å­—", False),
            (" ", False),
            ("ã€€", False),
            ("a#$", False),
            ("#ã‚$", False),
            ("#1$", False),
            ("#ï¼¡$", False),
            ("", False),
            ("ã€Œã€", False),
            (" ---", False),
            (" #", False),
            (" ğŸ”¥", False),  # ã‚¹ãƒšãƒ¼ã‚¹ãŒå«ã¾ã‚Œã‚‹ã¨ False
        ]
        for token, expected in test_cases:
            with self.subTest(token=repr(token), expected=expected):
                actual = is_special_char_pattern(token)
                self.assertEqual(
                    actual,
                    expected,
                    f"ãƒˆãƒ¼ã‚¯ãƒ³ {repr(token)} ã® is_special_char_pattern çµæœãŒæœŸå¾…å€¤ã¨ç•°ãªã‚Šã¾ã™",
                )


# ----- ãƒ¡ã‚¤ãƒ³åˆ†æé–¢æ•°ã®ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ -----
# å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦åˆ†æçµæœã‚’æ¤œè¨¼
class AnalysisResultTests(unittest.TestCase):
    tokenizer = None  # ã‚¯ãƒ©ã‚¹å¤‰æ•°ã¨ã—ã¦ tokenizer ã‚’ä¿æŒ
    result = None
    stats = {}
    token_ids_by_category = {}
    details = {}

    @classmethod
    def setUpClass(cls):
        # ã“ã®ã‚¯ãƒ©ã‚¹ã®å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œå‰ã«ä¸€åº¦ã ã‘å®Ÿè¡Œ
        print(f"\n--- {cls.__name__} ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–‹å§‹ ---")
        print(f"ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ¢ãƒ‡ãƒ«: {TARGET_MODEL_ID}")
        try:
            # å®Ÿéš›ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ (ãƒ¢ãƒƒã‚¯ä¸ä½¿ç”¨)
            cls.tokenizer = transformers.AutoTokenizer.from_pretrained(
                TARGET_MODEL_ID, trust_remote_code=True
            )
            print(f"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ ({cls.tokenizer.__class__.__name__}) ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†")

            # å®Ÿéš›ã®åˆ†æã‚’å®Ÿè¡Œ (æ™‚é–“ãŒã‹ã‹ã‚‹å¯èƒ½æ€§ã‚ã‚Š)
            print(f"ãƒˆãƒ¼ã‚¯ãƒ³åˆ†æã‚’é–‹å§‹ã—ã¾ã™ (min_token_id={MIN_TEST_TOKEN_ID})...")
            cls.result = analyze_token_categories(
                TARGET_MODEL_ID, min_token_id=MIN_TEST_TOKEN_ID
            )
            print("ãƒˆãƒ¼ã‚¯ãƒ³åˆ†æå®Œäº†")

            # çµæœã®å–å¾—ã¨åŸºæœ¬çš„ãªãƒã‚§ãƒƒã‚¯
            if cls.result is None:
                raise RuntimeError("analyze_token_categories ãŒ None ã‚’è¿”ã—ã¾ã—ãŸ")
            cls.stats = cls.result.get("statistics", {})
            cls.token_ids_by_category = cls.result.get("token_ids", {})
            cls.details = cls.result.get("analysis_details", {})
            print(f"åˆ†æå¯¾è±¡ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {cls.details.get('num_tokens_analyzed', 0):,}")

        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã®å‡¦ç†
            print(f"\n****** ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­ã«è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ ******")
            print(f"ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}")
            print(f"ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {e}")
            import traceback

            print(traceback.format_exc())  # è©³ç´°ãªãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚’å‡ºåŠ›
            print("***************************************************\n")
            cls.result = None  # ã‚¨ãƒ©ãƒ¼ãƒ•ãƒ©ã‚°
        finally:
            print(f"--- {cls.__name__} ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº† ---")

    # --- åˆ†æçµæœã®åŸºæœ¬æ§‹é€ ãƒ»çµ±è¨ˆå€¤ãƒ»é›†åˆé–¢ä¿‚ã®ãƒ†ã‚¹ãƒˆ (å¤‰æ›´ãªã—) ---
    def test_åˆ†æçµæœã®åŸºæœ¬æ§‹é€ ã¨å¿…é ˆã‚­ãƒ¼ã®å­˜åœ¨ç¢ºèª(self):
        if self.result is None:
            self.fail("ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—ã®ãŸã‚ãƒ†ã‚¹ãƒˆå¼·åˆ¶çµ‚äº†")
        self.assertEqual(self.result["model_id"], TARGET_MODEL_ID)
        self.assertIsInstance(self.result["vocab_size"], int)
        self.assertGreaterEqual(self.result["vocab_size"], 0)
        self.assertIsInstance(self.result["num_special_tokens"], int)
        self.assertGreaterEqual(self.result["num_special_tokens"], 0)
        expected_top_keys = [
            "model_id",
            "vocab_size",
            "num_special_tokens",
            "analysis_details",
            "statistics",
            "token_ids",
        ]
        for key in expected_top_keys:
            self.assertIn(key, self.result)
        self.assertIsInstance(self.details, dict)
        expected_detail_keys = [
            "min_token_id_analyzed",
            "max_token_id_analyzed",
            "num_tokens_analyzed",
            "num_errors",
            "excluded_special_ids",
        ]
        for key in expected_detail_keys:
            self.assertIn(key, self.details)
        self.assertIsInstance(self.details["excluded_special_ids"], list)
        self.assertIsInstance(self.stats, dict)
        self.assertIsInstance(self.token_ids_by_category, dict)
        expected_category_keys = {
            "contains_japanese",
            "pure_japanese_script",
            "pure_english",
            "contains_hiragana",
            "contains_katakana_full",
            "contains_katakana_half",
            "contains_kanji",
            "contains_jp_punct_symbol",
            "contains_fullwidth_ascii",
            "contains_basic_english",
            "contains_digit",
            "special_char_pattern",
            "uncategorized",
        }
        self.assertEqual(set(self.stats.keys()), expected_category_keys)
        self.assertEqual(set(self.token_ids_by_category.keys()), expected_category_keys)
        self.assertGreaterEqual(self.details["num_tokens_analyzed"], 0)
        if self.details["num_tokens_analyzed"] == 0:
            logging.warning("åˆ†æå¯¾è±¡ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒ0ã§ã—ãŸã€‚")

    def test_ã‚«ãƒ†ã‚´ãƒªé–“ã®éƒ¨åˆ†é›†åˆé–¢ä¿‚ã®æ¤œè¨¼(self):
        if self.result is None:
            self.fail("ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—ã®ãŸã‚ãƒ†ã‚¹ãƒˆå¼·åˆ¶çµ‚äº†")

        def check_subset(subset_name, superset_name):
            subset_set = set(self.token_ids_by_category.get(subset_name, []))
            superset_set = set(self.token_ids_by_category.get(superset_name, []))
            if subset_set and superset_set:
                self.assertTrue(
                    subset_set.issubset(superset_set),
                    f"'{subset_name}' ã¯ '{superset_name}' ã®éƒ¨åˆ†é›†åˆã§ã‚ã‚‹ã¹ãã§ã™",
                )

        check_subset("pure_japanese_script", "contains_japanese")
        check_subset("contains_hiragana", "contains_japanese")
        check_subset("contains_katakana_full", "contains_japanese")
        check_subset("contains_katakana_half", "contains_japanese")
        check_subset("contains_kanji", "contains_japanese")
        check_subset("contains_jp_punct_symbol", "contains_japanese")
        check_subset("contains_fullwidth_ascii", "contains_japanese")
        check_subset("pure_english", "contains_basic_english")
        pure_jp_set = set(self.token_ids_by_category.get("pure_japanese_script", []))
        jp_ps_set = set(self.token_ids_by_category.get("contains_jp_punct_symbol", []))
        fw_ascii_set = set(
            self.token_ids_by_category.get("contains_fullwidth_ascii", [])
        )
        if pure_jp_set and jp_ps_set:
            self.assertTrue(
                pure_jp_set.isdisjoint(jp_ps_set - pure_jp_set),
                "pure_japanese_script ã¨ (æ—¥æœ¬èªå¥èª­ç‚¹è¨˜å·ã®ã¿ã‚’å«ã‚€ãƒˆãƒ¼ã‚¯ãƒ³) ã¯æ’ä»–ã®ã¯ãšã§ã™",
            )
        if pure_jp_set and fw_ascii_set:
            self.assertTrue(
                pure_jp_set.isdisjoint(fw_ascii_set - pure_jp_set),
                "pure_japanese_script ã¨ (å…¨è§’ASCIIã®ã¿ã‚’å«ã‚€ãƒˆãƒ¼ã‚¯ãƒ³) ã¯æ’ä»–ã®ã¯ãšã§ã™",
            )

    def test_ã‚«ãƒ†ã‚´ãƒªé–“ã®æ’ä»–é–¢ä¿‚ã®æ¤œè¨¼(self):
        if self.result is None:
            self.fail("ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—ã®ãŸã‚ãƒ†ã‚¹ãƒˆå¼·åˆ¶çµ‚äº†")

        def check_disjoint(cat1_name, cat2_name):
            set1 = set(self.token_ids_by_category.get(cat1_name, []))
            set2 = set(self.token_ids_by_category.get(cat2_name, []))
            if set1 and set2:
                intersection = set1.intersection(set2)
                if intersection:
                    sample_size = 5
                    intersection_list = sorted(list(intersection))
                    sample_elements = intersection_list[:sample_size]
                    num_common = len(intersection_list)
                    error_msg = (
                        f"ã‚«ãƒ†ã‚´ãƒª '{cat1_name}' ã¨ '{cat2_name}' ã¯æ’ä»–ã§ã‚ã‚‹ã¹ãã§ã™ãŒã€{num_common} å€‹ã®å…±é€šè¦ç´ ã€‚\n"
                        f"  ã‚µãƒ³ãƒ—ãƒ«: {sample_elements}"
                    )
                    if self.tokenizer:
                        try:
                            decoded_samples = [
                                f"{tid}:{repr(self.tokenizer.decode([tid], clean_up_tokenization_spaces=False))}"
                                for tid in sample_elements
                            ]
                            error_msg += f"\n  ãƒ‡ã‚³ãƒ¼ãƒ‰ä¾‹: {decoded_samples}"
                        except Exception as e:
                            error_msg += f"\n  (ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e})"
                    self.fail(error_msg)

        check_disjoint("pure_japanese_script", "pure_english")
        check_disjoint("pure_japanese_script", "special_char_pattern")
        check_disjoint("pure_japanese_script", "uncategorized")
        check_disjoint("pure_english", "contains_japanese")
        check_disjoint("pure_english", "special_char_pattern")
        check_disjoint("pure_english", "uncategorized")
        check_disjoint("special_char_pattern", "contains_japanese")
        check_disjoint("special_char_pattern", "contains_basic_english")
        check_disjoint("special_char_pattern", "uncategorized")
        uncat_set = set(self.token_ids_by_category.get("uncategorized", []))
        if uncat_set:
            defined_categories = [
                "contains_japanese",
                "pure_japanese_script",
                "pure_english",
                "contains_hiragana",
                "contains_katakana_full",
                "contains_katakana_half",
                "contains_kanji",
                "contains_jp_punct_symbol",
                "contains_fullwidth_ascii",
                "contains_basic_english",
                "contains_digit",
                "special_char_pattern",
            ]
            for cat_name in defined_categories:
                check_disjoint("uncategorized", cat_name)

    def test_çµ±è¨ˆå€¤ã®æ•´åˆæ€§æ¤œè¨¼(self):
        if self.result is None:
            self.fail("ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—ã®ãŸã‚ãƒ†ã‚¹ãƒˆå¼·åˆ¶çµ‚äº†")
        for name, count in self.stats.items():
            self.assertEqual(
                count,
                len(self.token_ids_by_category.get(name, [])),
                f"ã‚«ãƒ†ã‚´ãƒª '{name}' ã®çµ±è¨ˆæ•°({count})ã¨IDãƒªã‚¹ãƒˆé•·({len(self.token_ids_by_category.get(name, []))})ãŒä¸€è‡´ã—ã¾ã›ã‚“",
            )
        all_categorized_ids_union = set()
        for name, ids in self.token_ids_by_category.items():
            if name != "uncategorized":
                all_categorized_ids_union.update(ids)
        num_analyzed = self.details.get("num_tokens_analyzed", 0)
        num_uncategorized = self.stats.get("uncategorized", 0)
        num_categorized_unique = len(all_categorized_ids_union)
        self.assertEqual(
            num_analyzed,
            num_categorized_unique + num_uncategorized,
            f"åˆ†æãƒˆãƒ¼ã‚¯ãƒ³æ•°({num_analyzed:,})ãŒã€åˆ†é¡æ¸ˆã¿ãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°({num_categorized_unique:,}) + æœªåˆ†é¡æ•°({num_uncategorized:,}) = {num_categorized_unique + num_uncategorized:,} ã¨ä¸€è‡´ã—ã¾ã›ã‚“",
        )

    # --- åˆ†é¡ãƒ­ã‚¸ãƒƒã‚¯ã®ä¸€è²«æ€§æ¤œè¨¼ãƒ†ã‚¹ãƒˆ ---
    def test_åˆ†é¡ãƒ­ã‚¸ãƒƒã‚¯ã®ä¸€è²«æ€§æ¤œè¨¼_ã‚µãƒ³ãƒ—ãƒ«(self):
        """
        å®Ÿéš›ã®åˆ†æçµæœã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€å„ãƒˆãƒ¼ã‚¯ãƒ³ã«ã¤ã„ã¦
        åˆ†é¡ãƒ­ã‚¸ãƒƒã‚¯ã‚’å†å®Ÿè¡Œã—ãŸçµæœã¨ã€å®Ÿéš›ã®åˆ†æçµæœãŒä¸€è‡´ã™ã‚‹ã‹æ¤œè¨¼ã™ã‚‹ã€‚
        """
        if self.result is None:
            self.fail("ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—ã®ãŸã‚ãƒ†ã‚¹ãƒˆå¼·åˆ¶çµ‚äº†")
        if self.tokenizer is None:
            self.fail("TokenizerãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        if self.details["num_tokens_analyzed"] == 0:
            self.skipTest("åˆ†æå¯¾è±¡ãƒˆãƒ¼ã‚¯ãƒ³ãŒãªã„ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")

        sampled_token_ids = set()
        categories_to_sample = [
            "pure_japanese_script",
            "pure_english",
            "special_char_pattern",
            "uncategorized",
            "contains_katakana_half",
            "contains_fullwidth_ascii",
            "contains_jp_punct_symbol",
            "contains_digit",
        ]
        for cat_name in categories_to_sample:
            ids_in_category = self.token_ids_by_category.get(cat_name, [])
            if ids_in_category:
                k = min(len(ids_in_category), MAX_SAMPLES_PER_CATEGORY)
                sampled_token_ids.update(random.sample(ids_in_category, k))

        final_sample_ids = sorted(list(sampled_token_ids))
        if len(final_sample_ids) > TOTAL_SAMPLES_FOR_LOGIC_TEST:
            final_sample_ids = random.sample(
                final_sample_ids, TOTAL_SAMPLES_FOR_LOGIC_TEST
            )
            final_sample_ids.sort()

        if not final_sample_ids:
            self.skipTest("æ¤œè¨¼å¯¾è±¡ã®ã‚µãƒ³ãƒ—ãƒ«ãƒˆãƒ¼ã‚¯ãƒ³IDãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

        print(
            f"\n--- åˆ†é¡ãƒ­ã‚¸ãƒƒã‚¯ä¸€è²«æ€§æ¤œè¨¼ (ã‚µãƒ³ãƒ—ãƒ«IDæ•°: {len(final_sample_ids)}) ---"
        )

        for token_id in final_sample_ids:
            try:
                decoded_token = self.tokenizer.decode(
                    [token_id], clean_up_tokenization_spaces=False
                )
            except Exception as e:
                self.fail(f"ID {token_id} ã®ãƒ‡ã‚³ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                continue

            with self.subTest(token_id=token_id, decoded_token=repr(decoded_token)):
                expected_cats_for_this_token = self._calculate_expected_categories(
                    decoded_token
                )
                actual_cats_for_this_token = set()
                for cat_name, ids_list in self.token_ids_by_category.items():
                    # IDãƒªã‚¹ãƒˆãŒå¤§ãããªã‚‹å¯èƒ½æ€§ã‚’è€ƒæ…®ã—ã€setå¤‰æ›ã¯ä¸€åº¦ã ã‘è¡Œã†ã‹ã€
                    # ã‚‚ã—ãã¯ analyze_token_categories å´ã§çµæœã‚’setã§è¿”ã™ã‚ˆã†ã«å¤‰æ›´æ¤œè¨
                    if token_id in set(ids_list):
                        actual_cats_for_this_token.add(cat_name)

                self.assertSetEqual(
                    actual_cats_for_this_token,
                    expected_cats_for_this_token,
                    f"ID {token_id} ({repr(decoded_token)}) ã®åˆ†é¡çµæœãŒãƒ­ã‚¸ãƒƒã‚¯ã¨ä¸€è‡´ã—ã¾ã›ã‚“",
                )

    # --- æ–°ã—ã„ãƒ†ã‚¹ãƒˆãƒ¡ã‚½ãƒƒãƒ‰: Partialæ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒ³ã®æ¤œè¨¼ ---
    def test_partial_japanese_token_samples(self):
        """
        `contains_japanese` ã«å«ã¾ã‚Œã€ã‹ã¤ `pure_japanese_script` ã«ã¯å«ã¾ã‚Œãªã„
        ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆéƒ¨åˆ†ãƒˆãƒ¼ã‚¯ãƒ³ã‚„æ··åˆãƒˆãƒ¼ã‚¯ãƒ³ã®å€™è£œï¼‰ãŒå®Ÿéš›ã« `contains_japanese`
        ã«æ­£ã—ãåˆ†é¡ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ã‚µãƒ³ãƒ—ãƒ«æ¤œè¨¼ã™ã‚‹ã€‚
        """
        if self.result is None:
            self.fail("ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—ã®ãŸã‚ãƒ†ã‚¹ãƒˆå¼·åˆ¶çµ‚äº†")
        if self.tokenizer is None:
            self.fail("TokenizerãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“")

        contains_jp_set = set(self.token_ids_by_category.get("contains_japanese", []))
        pure_jp_set = set(self.token_ids_by_category.get("pure_japanese_script", []))

        partial_or_mixed_jp_ids = list(contains_jp_set - pure_jp_set)

        if not partial_or_mixed_jp_ids:
            self.skipTest("Partial/Mixedæ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒ³ã®å€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

        num_samples = min(len(partial_or_mixed_jp_ids), TOTAL_SAMPLES_FOR_LOGIC_TEST)
        sampled_ids = random.sample(partial_or_mixed_jp_ids, num_samples)
        sampled_ids.sort()

        print(
            f"\n--- Partial/Mixedæ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒ³æ¤œè¨¼ (ã‚µãƒ³ãƒ—ãƒ«IDæ•°: {len(sampled_ids)}) ---"
        )

        for token_id in sampled_ids:
            try:
                decoded_token = self.tokenizer.decode(
                    [token_id], clean_up_tokenization_spaces=False
                )
            except Exception as e:
                logging.warning(
                    f"ID {token_id} ã®ãƒ‡ã‚³ãƒ¼ãƒ‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ (Partial/Mixedæ¤œè¨¼): {e}"
                )
                continue

            with self.subTest(token_id=token_id, decoded_token=repr(decoded_token)):
                self.assertIn(
                    token_id,
                    contains_jp_set,
                    f"ID {token_id} ({repr(decoded_token)}) ã¯ partial/mixed å€™è£œã§ã‚ã‚Šã€'contains_japanese' ã«å«ã¾ã‚Œã‚‹ã¹ãã§ã™",
                )
                self.assertNotIn(
                    token_id,
                    pure_jp_set,
                    f"ID {token_id} ({repr(decoded_token)}) ã¯ partial/mixed å€™è£œã§ã‚ã‚Šã€'pure_japanese_script' ã«å«ã¾ã‚Œã‚‹ã¹ãã§ã¯ã‚ã‚Šã¾ã›ã‚“",
                )
                has_jp_related_flag = False
                for char in decoded_token:
                    if is_japanese_related_char(char):
                        has_jp_related_flag = True
                        break
                self.assertTrue(
                    has_jp_related_flag,
                    f"ID {token_id} ({repr(decoded_token)}) ã¯ãƒ‡ã‚³ãƒ¼ãƒ‰çµæœã«æ—¥æœ¬èªé–¢é€£æ–‡å­—ã‚’å«ã‚€ã¯ãšã§ã™ (contains_japaneseåˆ¤å®š)",
                )

    # --- ãƒ˜ãƒ«ãƒ‘ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ (_calculate_expected_categories, _calculate_token_flags) ---
    def _calculate_expected_categories(self, decoded_token):
        """
        ä¸ãˆã‚‰ã‚ŒãŸãƒ‡ã‚³ãƒ¼ãƒ‰æ¸ˆã¿ãƒˆãƒ¼ã‚¯ãƒ³æ–‡å­—åˆ—ã«å¯¾ã—ã¦ã€
        analyze_token_categories ã¨åŒã˜åˆ†é¡ãƒ­ã‚¸ãƒƒã‚¯ã‚’é©ç”¨ã—ã€
        å±ã™ã‚‹ã¨æœŸå¾…ã•ã‚Œã‚‹ã‚«ãƒ†ã‚´ãƒªåã®ã‚»ãƒƒãƒˆã‚’è¿”ã™ã€‚
        """
        if not decoded_token:
            return set()
        flags = self._calculate_token_flags(decoded_token)
        expected_cats = set()
        if flags["is_related_to_jp"]:
            expected_cats.add("contains_japanese")
        if flags["has_hiragana"]:
            expected_cats.add("contains_hiragana")
        if flags["has_katakana_full"]:
            expected_cats.add("contains_katakana_full")
        if flags["has_katakana_half"]:
            expected_cats.add("contains_katakana_half")
        if flags["has_kanji"]:
            expected_cats.add("contains_kanji")
        if flags["has_jp_punct_symbol"]:
            expected_cats.add("contains_jp_punct_symbol")
        if flags["has_fullwidth_ascii"]:
            expected_cats.add("contains_fullwidth_ascii")
        if flags["all_pure_jp_script"] and (
            flags["has_hiragana"]
            or flags["has_katakana_full"]
            or flags["has_katakana_half"]
            or flags["has_kanji"]
        ):
            expected_cats.add("pure_japanese_script")
        if flags["has_basic_english"]:
            expected_cats.add("contains_basic_english")
        if flags["all_basic_english"] and not flags["is_related_to_jp"]:
            expected_cats.add("pure_english")
        if flags["has_digit"]:
            expected_cats.add("contains_digit")
        is_sp_pattern = is_special_char_pattern(decoded_token)
        if (
            not flags["is_related_to_jp"]
            and not flags["has_basic_english"]
            and not flags["has_digit"]
            and is_sp_pattern
        ):
            expected_cats.add("special_char_pattern")
        # is_related_to_jp, has_basic_english, has_digit, is_sp_pattern ã®ã„ãšã‚Œã‚‚Falseã®å ´åˆã«uncategorized
        if (
            not flags["is_related_to_jp"]
            and not flags["has_basic_english"]
            and not flags["has_digit"]
            and not is_sp_pattern
        ):
            expected_cats.add("uncategorized")
        return expected_cats

    def _calculate_token_flags(self, decoded_token):
        """
        analyze_token_categories å†…ã®æ–‡å­—ãƒã‚§ãƒƒã‚¯ãƒ­ã‚¸ãƒƒã‚¯ã‚’æ¨¡å€£ã—ã€
        ãƒˆãƒ¼ã‚¯ãƒ³ã«é–¢ã™ã‚‹ãƒ•ãƒ©ã‚°è¾æ›¸ã‚’è¿”ã™ã€‚
        """
        flags = {
            "has_hiragana": False,
            "has_katakana_full": False,
            "has_katakana_half": False,
            "has_kanji": False,
            "has_jp_punct_symbol": False,
            "has_fullwidth_ascii": False,
            "has_basic_english": False,
            "has_digit": False,
            "has_other_char": False,
            "all_pure_jp_script": True,
            "all_basic_english": True,
            "is_related_to_jp": False,
        }
        if not decoded_token:
            return flags
        for char in decoded_token:
            is_hira = char in HIRAGANA
            is_kata_f = char in KATAKANA
            is_kata_h = char in KATAKANA_HW
            is_kanji = char in KANJI_COMMON or char in KANJI_EXT_A
            is_jp_ps = char in JP_PUNCT or char in JP_SYMBOLS_ETC
            is_fw_ascii = char in JP_FULLWIDTH_ASCII_PRINTABLE
            is_basic_eng = char in ENGLISH_BASIC
            is_digit = "0" <= char <= "9"
            is_space = char.isspace()
            if is_hira:
                flags["has_hiragana"] = True
            if is_kata_f:
                flags["has_katakana_full"] = True
            if is_kata_h:
                flags["has_katakana_half"] = True
            if is_kanji:
                flags["has_kanji"] = True
            if is_jp_ps:
                flags["has_jp_punct_symbol"] = True
            if is_fw_ascii:
                flags["has_fullwidth_ascii"] = True
            if is_basic_eng:
                flags["has_basic_english"] = True
            if is_digit:
                flags["has_digit"] = True
            is_pure_jp = is_hira or is_kata_f or is_kata_h or is_kanji
            if not is_pure_jp:
                flags["all_pure_jp_script"] = False
            if not is_basic_eng:
                flags["all_basic_english"] = False
            is_jp_related_char_flag = is_pure_jp or is_jp_ps or is_fw_ascii
            if is_jp_related_char_flag:
                flags["is_related_to_jp"] = True
            if not (
                is_jp_related_char_flag
                or is_basic_eng
                or is_digit
                or is_space
                or char.isalnum()
            ):
                # ç‰¹æ®Šæ–‡å­—ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å±ã•ãªã„ãã®ä»–ã®æ–‡å­—ãŒã‚ã‚‹ã‹
                if not is_special_char_pattern(char):
                    flags["has_other_char"] = True
        return flags


# --- ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ ---
if __name__ == "__main__":
    # verbosity=2 ã§å„ãƒ†ã‚¹ãƒˆãƒ¡ã‚½ãƒƒãƒ‰åã‚‚è¡¨ç¤º
    unittest.main(verbosity=2)
