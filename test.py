#!/usr/bin/env python
# -*- coding: utf-8 -*-
import unittest
import logging
import transformers
import os
import random  # ã‚µãƒ³ãƒ—ãƒ«æŠ½å‡ºã«ä½¿ç”¨
from token_analyzer_jp import (
    # åˆ©ç”¨ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
    is_japanese_related_char,
    is_pure_japanese_script_char,
    is_special_char_pattern,
    # ãƒ¡ã‚¤ãƒ³åˆ†æé–¢æ•°
    analyze_token_categories,
    # å®šç¾©ã•ã‚ŒãŸæ–‡å­—ã‚»ãƒƒãƒˆï¼ˆåˆ†é¡ãƒ­ã‚¸ãƒƒã‚¯å†ç¾ã®ãŸã‚ï¼‰
    HIRAGANA,
    KATAKANA,
    KATAKANA_HW,
    KANJI_COMMON,
    KANJI_EXT_A,
    JP_PUNCT,
    JP_SYMBOLS_ETC,
    JP_FULLWIDTH_ASCII_PRINTABLE,
    ENGLISH_BASIC,
)

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ã®ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«è¨­å®š (å¿…è¦ã«å¿œã˜ã¦å¤‰æ›´)
# logging.disable(logging.CRITICAL) # ãƒ­ã‚°ã‚’æŠ‘åˆ¶ã™ã‚‹å ´åˆ
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)  # INFOãƒ¬ãƒ™ãƒ«ä»¥ä¸Šã‚’è¡¨ç¤º

# --- å®šæ•° ---
TARGET_MODEL_ID = "unsloth/Llama-4-Scout-17B-16E-Instruct"
MIN_TEST_TOKEN_ID = 102
MAX_SAMPLES_PER_CATEGORY = 5
TOTAL_SAMPLES_FOR_LOGIC_TEST = 50


# --- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•° (åˆ†é¡ãƒ­ã‚¸ãƒƒã‚¯å†ç¾ç”¨ãƒ˜ãƒ«ãƒ‘ãƒ¼) ---
def _calculate_token_flags_util(decoded_token):
    """ãƒˆãƒ¼ã‚¯ãƒ³æ–‡å­—åˆ—ã‹ã‚‰æ–‡å­—ç¨®ãƒ•ãƒ©ã‚°ã‚’è¨ˆç®—ã™ã‚‹ (ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ç‰ˆ)"""
    flags = {
        "has_hiragana": False,
        "has_katakana_full": False,
        "has_katakana_half": False,
        "has_kanji": False,
        "has_jp_punct_symbol": False,
        "has_fullwidth_ascii": False,
        "has_basic_english": False,
        "has_digit": False,
        "has_other_char": False,
        "all_pure_jp_script": True,
        "all_basic_english": True,
        "is_related_to_jp": False,
    }
    if not decoded_token:
        return flags
    for char in decoded_token:
        is_hira = char in HIRAGANA
        is_kata_f = char in KATAKANA
        is_kata_h = char in KATAKANA_HW
        is_kanji = char in KANJI_COMMON or char in KANJI_EXT_A
        is_jp_ps = char in JP_PUNCT or char in JP_SYMBOLS_ETC
        is_fw_ascii = char in JP_FULLWIDTH_ASCII_PRINTABLE
        is_basic_eng = char in ENGLISH_BASIC
        is_digit_char = "0" <= char <= "9"
        is_space = char.isspace()
        if is_hira:
            flags["has_hiragana"] = True
        if is_kata_f:
            flags["has_katakana_full"] = True
        if is_kata_h:
            flags["has_katakana_half"] = True
        if is_kanji:
            flags["has_kanji"] = True
        if is_jp_ps:
            flags["has_jp_punct_symbol"] = True
        if is_fw_ascii:
            flags["has_fullwidth_ascii"] = True
        if is_basic_eng:
            flags["has_basic_english"] = True
        if is_digit_char:
            flags["has_digit"] = True
        is_pure_jp = is_hira or is_kata_f or is_kata_h or is_kanji
        if not is_pure_jp:
            flags["all_pure_jp_script"] = False
        if not is_basic_eng:
            flags["all_basic_english"] = False
        is_jp_related_char_flag = is_pure_jp or is_jp_ps or is_fw_ascii
        if is_jp_related_char_flag:
            flags["is_related_to_jp"] = True
        is_potentially_special = not (
            char.isalnum()
            or is_space
            or is_jp_related_char_flag
            or is_basic_eng
            or is_digit_char
        )
        if is_potentially_special:
            if not is_special_char_pattern(char):
                flags["has_other_char"] = True
    return flags


def _calculate_expected_categories_util(decoded_token):
    """åˆ†é¡ãƒ­ã‚¸ãƒƒã‚¯ã«åŸºã¥ãã€ãƒˆãƒ¼ã‚¯ãƒ³ãŒå±ã™ã¹ãã‚«ãƒ†ã‚´ãƒªã®ã‚»ãƒƒãƒˆã‚’è¨ˆç®—ã™ã‚‹ (ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°ç‰ˆ)"""
    if not decoded_token:
        return set()
    flags = _calculate_token_flags_util(decoded_token)
    expected_cats = set()
    if flags["is_related_to_jp"]:
        expected_cats.add("contains_japanese")
    if flags["has_hiragana"]:
        expected_cats.add("contains_hiragana")
    if flags["has_katakana_full"]:
        expected_cats.add("contains_katakana_full")
    if flags["has_katakana_half"]:
        expected_cats.add("contains_katakana_half")
    if flags["has_kanji"]:
        expected_cats.add("contains_kanji")
    if flags["has_jp_punct_symbol"]:
        expected_cats.add("contains_jp_punct_symbol")
    if flags["has_fullwidth_ascii"]:
        expected_cats.add("contains_fullwidth_ascii")
    if flags["all_pure_jp_script"] and (
        flags["has_hiragana"]
        or flags["has_katakana_full"]
        or flags["has_katakana_half"]
        or flags["has_kanji"]
    ):
        expected_cats.add("pure_japanese_script")
    if flags["has_basic_english"]:
        expected_cats.add("contains_basic_english")
    if flags["all_basic_english"] and not flags["is_related_to_jp"]:
        expected_cats.add("pure_english")
    if flags["has_digit"]:
        expected_cats.add("contains_digit")
    is_sp_pattern = is_special_char_pattern(decoded_token)
    if (
        not flags["is_related_to_jp"]
        and not flags["has_basic_english"]
        and not flags["has_digit"]
        and is_sp_pattern
    ):
        expected_cats.add("special_char_pattern")
    if (
        not flags["is_related_to_jp"]
        and not flags["has_basic_english"]
        and not flags["has_digit"]
        and not is_sp_pattern
    ):
        expected_cats.add("uncategorized")
    return expected_cats


# ----- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°ã®å˜ä½“ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ -----
class HelperFunctionTests(unittest.TestCase):
    def test_is_japanese_related_charã®æ‹¡å¼µã‚±ãƒ¼ã‚¹(self):
        test_cases = [
            ("ã‚", True),
            ("ã‚¢", True),
            ("æ¼¢", True),
            ("ãƒ¼", True),
            ("ï½¶", True),
            ("ï¾Ÿ", True),
            ("ã€‚", True),
            ("ã€€", True),
            ("ãƒ»", True),
            ("ï¿¥", True),
            ("ã€Œ", True),
            ("ï½¤", True),
            ("ï¼¡", True),
            ("ï½‚", True),
            ("ï¼", True),
            ("ï¼", True),
            ("ï½", True),
            ("A", False),
            ("1", False),
            ("$", False),
            (" ", False),
            ("\n", False),
            ("Î±", False),
        ]
        for char, expected in test_cases:
            with self.subTest(char=char, expected=expected):
                actual = is_japanese_related_char(char)
                self.assertEqual(
                    actual,
                    expected,
                    f"æ–‡å­— '{char}' (U+{ord(char):04X}) is_japanese_related_char",
                )

    def test_is_pure_japanese_script_charã®æ‹¡å¼µã‚±ãƒ¼ã‚¹(self):
        test_cases = [
            ("ã‚", True),
            ("ã‚¢", True),
            ("æ¼¢", True),
            ("ï½¶", True),
            ("ãƒ¼", True),
            ("ï½¥", True),
            ("ï¨‘", False),
            ("ã€‚", False),
            ("ã€€", False),
            ("ï¼¡", False),
            ("1", False),
            (" ", False),
            ("\n", False),
        ]
        for char, expected in test_cases:
            with self.subTest(char=char, expected=expected):
                actual = is_pure_japanese_script_char(char)
                self.assertEqual(
                    actual,
                    expected,
                    f"æ–‡å­— '{char}' (U+{ord(char):04X}) is_pure_japanese_script_char",
                )

    def test_is_special_char_patternã®æ‹¡å¼µã‚±ãƒ¼ã‚¹(self):
        test_cases = [
            ("!!!", True),
            ("@#$", True),
            ("&&&", True),
            ("+-*/", True),
            ("---", True),
            ("===", True),
            ("abc", False),
            ("ã‚ã„ã†", False),
            ("123", False),
            ("ï¼¡ï¼¢ï¼£", False),
            ("ã‚«ã‚¿ã‚«ãƒŠ", False),
            ("åŠè§’ï½¶ï¾…", False),
            ("æ¼¢å­—", False),
            (" ", False),
            ("ã€€", False),
            ("a#$", False),
            ("#ã‚$", False),
            ("#1$", False),
            ("#ï¼¡$", False),
            ("", False),
            ("ã€Œã€", False),
            (" ---", False),
            (" #", False),
            (" ğŸ”¥", False),
        ]
        for token, expected in test_cases:
            with self.subTest(token=repr(token), expected=expected):
                actual = is_special_char_pattern(token)
                self.assertEqual(
                    actual, expected, f"ãƒˆãƒ¼ã‚¯ãƒ³ {repr(token)} is_special_char_pattern"
                )


# ----- åˆ†é¡ãƒ­ã‚¸ãƒƒã‚¯ã®å˜ä½“ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ -----
class LogicVerificationTests(unittest.TestCase):
    def test_characteristic_patterns_logic(self):
        """å®šç¾©ã•ã‚ŒãŸç‰¹å¾´çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã¤ã„ã¦ã€åˆ†é¡ãƒ­ã‚¸ãƒƒã‚¯ãŒæ­£ã—ãåƒãã‹æ¤œè¨¼ã™ã‚‹ã€‚"""
        # (èª¬æ˜, ãƒã‚¸ãƒ†ã‚£ãƒ–ä¾‹, ãƒã‚¬ãƒ†ã‚£ãƒ–ä¾‹, æœŸå¾…ã‚«ãƒ†ã‚´ãƒª(ãƒã‚¸ãƒ†ã‚£ãƒ–), é™¤å¤–ã‚«ãƒ†ã‚´ãƒª(ãƒã‚¸ãƒ†ã‚£ãƒ–), ãƒã‚¬ãƒ†ã‚£ãƒ–ä¾‹ã§é™¤å¤–ã™ã¹ãã‚µãƒ–ã‚«ãƒ†ã‚´ãƒª)
        # èª¬æ˜éƒ¨åˆ†ã‚’æ—¥æœ¬èªã«ä¿®æ­£
        test_patterns = [
            (
                "åŠè§’ã‚«ã‚¿ã‚«ãƒŠ",
                "ï¾ƒï½½ï¾„",
                "ãƒ†ã‚¹ãƒˆ",
                {"contains_japanese", "contains_katakana_half", "pure_japanese_script"},
                {"contains_katakana_full", "pure_english"},
                "contains_katakana_half",
            ),
            (
                "å…¨è§’è‹±å­—",
                "ï¼¡ï¼¢ï¼£",
                "ABC",
                {"contains_japanese", "contains_fullwidth_ascii"},
                {"pure_japanese_script", "pure_english"},
                "contains_fullwidth_ascii",
            ),
            (
                "å…¨è§’æ•°å­—",
                "ï¼‘ï¼’ï¼“",
                "123",
                {"contains_japanese", "contains_fullwidth_ascii"},
                {"pure_japanese_script", "pure_english", "contains_digit"},
                "contains_fullwidth_ascii",
            ),
            (
                "å…¨è§’è¨˜å·",
                "ï¼…ï¼†ï¼",
                "%&!",
                {
                    "contains_japanese",
                    "contains_fullwidth_ascii",
                    "contains_jp_punct_symbol",
                },
                {"pure_japanese_script", "pure_english"},
                "contains_fullwidth_ascii",
            ),
            (
                "ç´”ç²‹ã²ã‚‰ãŒãª",
                "ã‚ã„ã†ãˆãŠ",
                "ã‚¢ã‚¤ã‚¦ã‚¨ã‚ª",
                {"contains_japanese", "contains_hiragana", "pure_japanese_script"},
                {"contains_katakana_full", "pure_english"},
                "contains_hiragana",
            ),
            (
                "ç´”ç²‹ã‚«ã‚¿ã‚«ãƒŠ(é•·éŸ³ç¬¦å«)",
                "ãƒˆãƒ¼ã‚¯ãƒ³",
                "token",
                {"contains_japanese", "contains_katakana_full", "pure_japanese_script"},
                {"contains_hiragana", "pure_english"},
                "contains_katakana_full",
            ),
            (
                "ç´”ç²‹æ¼¢å­—",
                "æ—¥æœ¬èª",
                "ã«ã»ã‚“ã”",
                {"contains_japanese", "contains_kanji", "pure_japanese_script"},
                {"contains_hiragana", "pure_english"},
                "contains_kanji",
            ),
            (
                "ç´”ç²‹è‹±èª",
                "HelloWorld",
                "ãƒãƒ­ãƒ¼ãƒ¯ãƒ¼ãƒ«ãƒ‰",
                {"contains_basic_english", "pure_english"},
                {"contains_japanese"},
                "pure_english",
            ),
            (
                "ç‰¹æ®Šæ–‡å­—ãƒ‘ã‚¿ãƒ¼ãƒ³",
                "---",
                "-abc-",
                {"special_char_pattern"},
                {"contains_japanese", "contains_basic_english", "contains_digit"},
                "special_char_pattern",
            ),
            (
                "åŠè§’ã‚«ãƒŠ+è‹±èª",
                "ï¾ƒï½½ï¾„ABC",
                "ãƒ†ã‚¹ãƒˆABC",
                {
                    "contains_japanese",
                    "contains_katakana_half",
                    "contains_basic_english",
                },
                {"pure_japanese_script", "pure_english"},
                "contains_katakana_half",
            ),
            (
                "æ¼¢å­—+æ•°å­—(åŠè§’)",
                "æ±äº¬1",
                "Tokyo1",
                {"contains_japanese", "contains_kanji", "contains_digit"},
                {"pure_japanese_script", "pure_english"},
                "contains_kanji",
            ),
            (
                "ç©ºç™½ã®ã¿",
                "   ",
                "abc",
                {"uncategorized"},
                {"contains_japanese", "contains_basic_english", "special_char_pattern"},
                "uncategorized",
            ),
        ]
        print("\n--- ç‰¹å¾´ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†é¡ãƒ­ã‚¸ãƒƒã‚¯æ¤œè¨¼ ---")
        for (
            description,
            positive_example,
            negative_example,
            expected_cats_positive,
            excluded_cats_positive,
            neg_excluded_subcat,
        ) in test_patterns:
            with self.subTest(description=description, type="ãƒã‚¸ãƒ†ã‚£ãƒ–"):
                print(f"  æ¤œè¨¼: '{description}' - ãƒã‚¸ãƒ†ã‚£ãƒ–ä¾‹ '{positive_example}'")
                calculated_cats = _calculate_expected_categories_util(positive_example)
                self.assertSetEqual(
                    calculated_cats,
                    expected_cats_positive,
                    f"'{positive_example}' ã®ã‚«ãƒ†ã‚´ãƒªä¸ä¸€è‡´",
                )
                for excluded_cat in excluded_cats_positive:
                    self.assertNotIn(
                        excluded_cat,
                        calculated_cats,
                        f"'{positive_example}' ã¯ '{excluded_cat}' ã«å«ã¾ã‚Œã‚‹ã¹ãã§ãªã„",
                    )
            if negative_example:
                with self.subTest(description=description, type="ãƒã‚¬ãƒ†ã‚£ãƒ–"):
                    print(
                        f"  æ¤œè¨¼: '{description}' - ãƒã‚¬ãƒ†ã‚£ãƒ–ä¾‹ '{negative_example}'"
                    )
                    calculated_cats_neg = _calculate_expected_categories_util(
                        negative_example
                    )
                    if neg_excluded_subcat:
                        self.assertNotIn(
                            neg_excluded_subcat,
                            calculated_cats_neg,
                            f"ãƒã‚¬ãƒ†ã‚£ãƒ–ä¾‹ '{negative_example}' ã¯ã‚«ãƒ†ã‚´ãƒª '{neg_excluded_subcat}' ã«å«ã¾ã‚Œã‚‹ã¹ãã§ãªã„",
                        )


# ----- ãƒ¡ã‚¤ãƒ³åˆ†æé–¢æ•°ã®çµ±åˆãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ -----
class AnalysisIntegrationTests(unittest.TestCase):
    tokenizer = None
    result = None
    stats = {}
    token_ids_by_category = {}
    token_ids_by_category_sets = {}
    details = {}

    @classmethod
    def setUpClass(cls):
        print(f"\n--- {cls.__name__} ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–‹å§‹ ---")
        print(f"ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ¢ãƒ‡ãƒ«: {TARGET_MODEL_ID}")
        try:
            cls.tokenizer = transformers.AutoTokenizer.from_pretrained(
                TARGET_MODEL_ID, trust_remote_code=True
            )
            print(f"ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ ({cls.tokenizer.__class__.__name__}) ã®ãƒ­ãƒ¼ãƒ‰å®Œäº†")
            print(f"ãƒˆãƒ¼ã‚¯ãƒ³åˆ†æã‚’é–‹å§‹ã—ã¾ã™ (min_token_id={MIN_TEST_TOKEN_ID})...")
            cls.result = analyze_token_categories(
                TARGET_MODEL_ID, min_token_id=MIN_TEST_TOKEN_ID
            )
            print("ãƒˆãƒ¼ã‚¯ãƒ³åˆ†æå®Œäº†")
            if cls.result is None:
                raise RuntimeError("analyze_token_categories ãŒ None")
                cls.stats = cls.result.get("statistics", {})
                cls.token_ids_by_category = cls.result.get("token_ids", {})
                cls.token_ids_by_category_sets = {
                    name: set(ids) for name, ids in cls.token_ids_by_category.items()
                }
                cls.details = cls.result.get(
                    "analysis_details", {}
                )  # detailsãŒNoneã«ãªã‚‰ãªã„ã‚ˆã†ã«ä¿®æ­£
            print(
                f"åˆ†æå¯¾è±¡ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {cls.details.get('num_tokens_analyzed', 0):,}"
            )  # .get()ã‚’ä½¿ç”¨
        except Exception as e:
            print(
                f"\n****** ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­ã«è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼ ******\nã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—: {type(e).__name__}\nã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸: {e}"
            )
            import traceback

            print(traceback.format_exc())
            print("***********************************\n")
            cls.result = None
        finally:
            print(f"--- {cls.__name__} ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº† ---")

    def test_åˆ†æçµæœã®åŸºæœ¬æ§‹é€ ã¨å¿…é ˆã‚­ãƒ¼ã®å­˜åœ¨ç¢ºèª(self):
        if self.result is None:
            self.fail("ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—")
            return
        self.assertEqual(self.result["model_id"], TARGET_MODEL_ID)
        self.assertIsInstance(self.result["vocab_size"], int)
        self.assertGreaterEqual(self.result["vocab_size"], 0)
        self.assertIsInstance(self.result["num_special_tokens"], int)
        self.assertGreaterEqual(self.result["num_special_tokens"], 0)
        expected_top_keys = [
            "model_id",
            "vocab_size",
            "num_special_tokens",
            "analysis_details",
            "statistics",
            "token_ids",
        ]
        [self.assertIn(key, self.result) for key in expected_top_keys]
        self.assertIsInstance(self.details, dict)
        expected_detail_keys = [
            "min_token_id_analyzed",
            "max_token_id_analyzed",
            "num_tokens_analyzed",
            "num_errors",
            "excluded_special_ids",
        ]
        # --- detailsãŒç©ºã§ãªã„ã“ã¨ã‚’ç¢ºèªã—ã¦ã‹ã‚‰ã‚­ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯ ---
        if self.details:  # detailsãŒNoneã‚„ç©ºã§ãªã„å ´åˆã®ã¿ãƒã‚§ãƒƒã‚¯
            for key in expected_detail_keys:
                self.assertIn(key, self.details, f"details ã«ã‚­ãƒ¼ '{key}' ãŒã‚ã‚Šã¾ã›ã‚“")
            self.assertIsInstance(
                self.details.get("excluded_special_ids", []), list
            )  # .get() ã‚’ä½¿ç”¨
        else:
            self.fail(
                "analysis_details ãŒçµæœã«å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€ã¾ãŸã¯ç©ºã§ã™"
            )  # detailsãŒãªã‘ã‚Œã°å¤±æ•—
        self.assertIsInstance(self.stats, dict)
        self.assertIsInstance(self.token_ids_by_category, dict)
        expected_category_keys = {
            "contains_japanese",
            "pure_japanese_script",
            "pure_english",
            "contains_hiragana",
            "contains_katakana_full",
            "contains_katakana_half",
            "contains_kanji",
            "contains_jp_punct_symbol",
            "contains_fullwidth_ascii",
            "contains_basic_english",
            "contains_digit",
            "special_char_pattern",
            "uncategorized",
        }
        self.assertEqual(set(self.stats.keys()), expected_category_keys)
        self.assertEqual(set(self.token_ids_by_category.keys()), expected_category_keys)
        # --- .get() ã‚’ä½¿ç”¨ã—ã¦å®‰å…¨ã«ã‚¢ã‚¯ã‚»ã‚¹ ---
        self.assertGreaterEqual(
            self.details.get("num_tokens_analyzed", -1), 0
        )  # -1ã§åˆæœŸåŒ–ã—ã€0ä»¥ä¸Šã‹ã‚’ãƒã‚§ãƒƒã‚¯
        if self.details.get("num_tokens_analyzed", 0) == 0:
            logging.warning("åˆ†æå¯¾è±¡ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒ0ã§ã—ãŸã€‚")

    def test_ã‚«ãƒ†ã‚´ãƒªé–“ã®éƒ¨åˆ†é›†åˆé–¢ä¿‚ã®æ¤œè¨¼(self):
        if self.result is None:
            self.fail("ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—")
            return

        def check_subset(sub, super_):
            sub_set = self.token_ids_by_category_sets.get(sub, set())
            super_set = self.token_ids_by_category_sets.get(super_, set())
            self.assertTrue(
                sub_set.issubset(super_set), f"'{sub}' âŠ† '{super_}'"
            ) if sub_set or super_set else None  # ç©ºé›†åˆåŒå£«ã‚‚Trueãªã®ã§ãƒã‚§ãƒƒã‚¯ä¸è¦ã€ã©ã¡ã‚‰ã‹ã‚ã‚Œã°ãƒã‚§ãƒƒã‚¯

        check_subset("pure_japanese_script", "contains_japanese")
        check_subset("contains_hiragana", "contains_japanese")
        check_subset("contains_katakana_full", "contains_japanese")
        check_subset("contains_katakana_half", "contains_japanese")
        check_subset("contains_kanji", "contains_japanese")
        check_subset("contains_jp_punct_symbol", "contains_japanese")
        check_subset("contains_fullwidth_ascii", "contains_japanese")
        check_subset("pure_english", "contains_basic_english")
        pure_jp_set = self.token_ids_by_category_sets.get("pure_japanese_script", set())
        jp_ps_set = self.token_ids_by_category_sets.get(
            "contains_jp_punct_symbol", set()
        )
        fw_ascii_set = self.token_ids_by_category_sets.get(
            "contains_fullwidth_ascii", set()
        )
        if pure_jp_set and jp_ps_set:
            self.assertTrue(
                pure_jp_set.isdisjoint(jp_ps_set - pure_jp_set),
                "pure_jp ã¨ jp_punct_symbol(ã®ã¿) ã¯æ’ä»–",
            )
        if pure_jp_set and fw_ascii_set:
            self.assertTrue(
                pure_jp_set.isdisjoint(fw_ascii_set - pure_jp_set),
                "pure_jp ã¨ fw_ascii(ã®ã¿) ã¯æ’ä»–",
            )

    def test_ã‚«ãƒ†ã‚´ãƒªé–“ã®æ’ä»–é–¢ä¿‚ã®æ¤œè¨¼(self):
        if self.result is None:
            self.fail("ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—")
            return

        def check_disjoint(cat1, cat2):
            s1 = self.token_ids_by_category_sets.get(cat1, set())
            s2 = self.token_ids_by_category_sets.get(cat2, set())
            if s1 and s2:
                intersection = s1.intersection(s2)
                if intersection:
                    sample_size = 5
                    int_list = sorted(list(intersection))
                    samples = int_list[:sample_size]
                    num_common = len(int_list)
                    msg = f"æ’ä»–æ¤œè¨¼å¤±æ•—: '{cat1}'âˆ©'{cat2}'={num_common} å€‹å…±é€š\n ã‚µãƒ³ãƒ—ãƒ«:{samples}"
                    if self.tokenizer:
                        try:
                            decoded = [
                                f"{t}:{repr(self.tokenizer.decode([t], clean_up_tokenization_spaces=False))}"
                                for t in samples
                            ]
                            msg += f"\n ãƒ‡ã‚³ãƒ¼ãƒ‰ä¾‹:{decoded}"
                        except Exception as e:
                            msg += f"\n (ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼:{e})"
                    self.fail(msg)

        check_disjoint("pure_japanese_script", "pure_english")
        check_disjoint("pure_japanese_script", "special_char_pattern")
        check_disjoint("pure_japanese_script", "uncategorized")
        check_disjoint("pure_english", "contains_japanese")
        check_disjoint("pure_english", "special_char_pattern")
        check_disjoint("pure_english", "uncategorized")
        check_disjoint("special_char_pattern", "contains_japanese")
        check_disjoint("special_char_pattern", "contains_basic_english")
        check_disjoint("special_char_pattern", "uncategorized")
        if self.token_ids_by_category_sets.get("uncategorized", set()):
            defined_cats = [
                "contains_japanese",
                "pure_japanese_script",
                "pure_english",
                "contains_hiragana",
                "contains_katakana_full",
                "contains_katakana_half",
                "contains_kanji",
                "contains_jp_punct_symbol",
                "contains_fullwidth_ascii",
                "contains_basic_english",
                "contains_digit",
                "special_char_pattern",
            ]
            for cat in defined_cats:
                check_disjoint("uncategorized", cat)

    def test_çµ±è¨ˆå€¤ã®æ•´åˆæ€§æ¤œè¨¼(self):
        if self.result is None:
            self.fail("ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—")
            return
        for name, count in self.stats.items():
            self.assertEqual(
                count,
                len(self.token_ids_by_category.get(name, [])),
                f"çµ±è¨ˆå€¤ä¸ä¸€è‡´:{name}",
            )
        all_cat_ids = set().union(
            *[
                s
                for n, s in self.token_ids_by_category_sets.items()
                if n != "uncategorized"
            ]
        )
        num_analyzed = self.details.get("num_tokens_analyzed", 0)
        num_uncat = self.stats.get("uncategorized", 0)
        num_cat_unique = len(all_cat_ids)
        self.assertEqual(
            num_analyzed,
            num_cat_unique + num_uncat,
            f"åˆè¨ˆæ•°ä¸ä¸€è‡´:åˆ†æ={num_analyzed:,},åˆ†é¡æ¸ˆ={num_cat_unique:,},æœªåˆ†é¡={num_uncat:,}",
        )

    def test_åˆ†é¡ãƒ­ã‚¸ãƒƒã‚¯ã®ä¸€è²«æ€§æ¤œè¨¼_ã‚µãƒ³ãƒ—ãƒ«(self):
        if self.result is None:
            self.fail("ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—")
            return
        if self.tokenizer is None:
            self.fail("Tokenizeræœªãƒ­ãƒ¼ãƒ‰")
            return
        # --- .get() ã‚’ä½¿ç”¨ã—ã¦å®‰å…¨ã«ã‚¢ã‚¯ã‚»ã‚¹ ---
        if self.details.get("num_tokens_analyzed", 0) == 0:
            self.skipTest("åˆ†æå¯¾è±¡ãªã—")
            return
        sampled_ids = set()
        cats_to_sample = [
            "pure_japanese_script",
            "pure_english",
            "special_char_pattern",
            "uncategorized",
            "contains_katakana_half",
            "contains_fullwidth_ascii",
            "contains_jp_punct_symbol",
            "contains_digit",
        ]
        for cat in cats_to_sample:
            ids_cat = self.token_ids_by_category_sets.get(cat, set())
            if ids_cat:
                k = min(len(ids_cat), MAX_SAMPLES_PER_CATEGORY)
                sampled_ids.update(random.sample(list(ids_cat), k))
        final_ids = sorted(list(sampled_ids))
        if len(final_ids) > TOTAL_SAMPLES_FOR_LOGIC_TEST:
            final_ids = sorted(random.sample(final_ids, TOTAL_SAMPLES_FOR_LOGIC_TEST))
        if not final_ids:
            self.skipTest("æ¤œè¨¼ã‚µãƒ³ãƒ—ãƒ«IDãªã—")
            return
        print(f"\n--- åˆ†é¡ãƒ­ã‚¸ãƒƒã‚¯ä¸€è²«æ€§æ¤œè¨¼ (ã‚µãƒ³ãƒ—ãƒ«IDæ•°: {len(final_ids)}) ---")
        for tid in final_ids:
            try:
                decoded = self.tokenizer.decode(
                    [tid], clean_up_tokenization_spaces=False
                )
            except Exception as e:
                self.fail(f"ID {tid} ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
                continue
            with self.subTest(tid=tid, decoded=repr(decoded)):
                expected = _calculate_expected_categories_util(decoded)
                actual = {
                    name
                    for name, ids_set in self.token_ids_by_category_sets.items()
                    if tid in ids_set
                }
                self.assertSetEqual(
                    actual, expected, f"ID {tid} ({repr(decoded)}) åˆ†é¡ãƒ­ã‚¸ãƒƒã‚¯ä¸ä¸€è‡´"
                )

    def test_partial_japanese_token_samples(self):
        if self.result is None:
            self.fail("ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å¤±æ•—")
            return
        if self.tokenizer is None:
            self.fail("Tokenizeræœªãƒ­ãƒ¼ãƒ‰")
            return
        contains_jp = self.token_ids_by_category_sets.get("contains_japanese", set())
        pure_jp = self.token_ids_by_category_sets.get("pure_japanese_script", set())
        partial_mixed_ids = list(contains_jp - pure_jp)
        if not partial_mixed_ids:
            self.skipTest("Partial/Mixedå€™è£œãªã—")
            return
        num_samples = min(len(partial_mixed_ids), TOTAL_SAMPLES_FOR_LOGIC_TEST)
        sampled = sorted(random.sample(partial_mixed_ids, num_samples))
        print(
            f"\n--- Partial/Mixedæ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒ³æ¤œè¨¼ (ã‚µãƒ³ãƒ—ãƒ«IDæ•°: {len(sampled)}) ---"
        )
        for tid in sampled:
            try:
                decoded = self.tokenizer.decode(
                    [tid], clean_up_tokenization_spaces=False
                )
            except Exception as e:
                logging.warning(f"ID {tid} ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ (Partial/Mixedæ¤œè¨¼): {e}")
                continue
            with self.subTest(tid=tid, decoded=repr(decoded)):
                self.assertIn(
                    tid,
                    contains_jp,
                    f"ID {tid} ({repr(decoded)}) ã¯ contains_japanese ã«å«ã¾ã‚Œã‚‹ã¹ã",
                )
                self.assertNotIn(
                    tid,
                    pure_jp,
                    f"ID {tid} ({repr(decoded)}) ã¯ pure_japanese_script ã«å«ã¾ã‚Œãªã„ã¹ã",
                )
                has_jp_flag = any(is_japanese_related_char(c) for c in decoded)
                self.assertTrue(
                    has_jp_flag,
                    f"ID {tid} ({repr(decoded)}) ã¯æ—¥æœ¬èªé–¢é€£æ–‡å­—ã‚’å«ã‚€ã¹ã",
                )


# --- ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ ---
if __name__ == "__main__":
    suite = unittest.TestSuite()
    suite.addTest(unittest.makeSuite(HelperFunctionTests))
    suite.addTest(unittest.makeSuite(LogicVerificationTests))
    suite.addTest(unittest.makeSuite(AnalysisIntegrationTests))
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    # if not result.wasSuccessful(): exit(1) # CIç”¨
